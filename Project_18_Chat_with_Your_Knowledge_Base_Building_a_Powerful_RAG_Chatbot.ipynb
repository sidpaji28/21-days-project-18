{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up the LLM"
      ],
      "metadata": {
        "id": "ZWK-qrtRn-v-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade langchain\n",
        "!pip install --upgrade langchain-core\n",
        "!pip install --upgrade langchain-community\n",
        "!pip install --upgrade langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVK_hO-EU99g",
        "outputId": "36abde50-8971-4935-de1a-d9786304ef50",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.77)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.5)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.77)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.11.9)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.3.1)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.12/dist-packages (0.3.30)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.77)\n",
            "Requirement already satisfied: langchain<2.0.0,>=0.3.27 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.3.27)\n",
            "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.43)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.5 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.32.5)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (3.12.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.10.1)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.1.125 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.28)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.12/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain<2.0.0,>=0.3.27->langchain-community) (2.11.9)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (1.33)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=0.3.75->langchain-community) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.1.125->langchain-community) (0.25.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.5->langchain-community) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=0.3.75->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<2.0.0,>=0.3.27->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: langchain-google-genai in /usr/local/lib/python3.12/dist-packages (2.1.12)\n",
            "Requirement already satisfied: langchain-core>=0.3.75 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.3.77)\n",
            "Requirement already satisfied: google-ai-generativelanguage<1,>=0.7 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (2.11.9)\n",
            "Requirement already satisfied: filetype<2,>=1.2 in /usr/local/lib/python3.12/dist-packages (from langchain-google-genai) (1.2.0)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.25.1)\n",
            "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.29.5)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (1.33)\n",
            "Requirement already satisfied: PyYAML<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (4.15.0)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.75->langchain-google-genai) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2->langchain-google-genai) (0.4.1)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.32.5)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.75.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (4.9.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.75->langchain-google-genai) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (0.16.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1,>=0.7->langchain-google-genai) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.75->langchain-google-genai) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "help(ChatGoogleGenerativeAI)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AULGUVyIFQ70",
        "outputId": "4c596226-8c26-4f84-8507-a4a82725a234"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class ChatGoogleGenerativeAI in module langchain_google_genai.chat_models:\n",
            "\n",
            "class ChatGoogleGenerativeAI(langchain_google_genai._common._BaseGoogleGenerativeAI, langchain_core.language_models.chat_models.BaseChatModel)\n",
            " |  ChatGoogleGenerativeAI(*, name: Optional[str] = None, cache: Union[langchain_core.caches.BaseCache, bool, NoneType] = None, verbose: bool = <factory>, callbacks: Union[list[langchain_core.callbacks.base.BaseCallbackHandler], langchain_core.callbacks.base.BaseCallbackManager, NoneType] = None, tags: Optional[list[str]] = None, metadata: Optional[dict[str, Any]] = None, custom_get_token_ids: Optional[Callable[[str], list[int]]] = None, callback_manager: Optional[langchain_core.callbacks.base.BaseCallbackManager] = None, rate_limiter: Optional[langchain_core.rate_limiters.BaseRateLimiter] = None, disable_streaming: Union[bool, Literal['tool_calling']] = False, model: str, api_key: Optional[pydantic.types.SecretStr] = <factory>, credentials: Any = None, temperature: float = 0.7, top_p: Optional[float] = None, top_k: Optional[int] = None, max_tokens: Optional[int] = None, n: int = 1, retries: int = 6, request_timeout: Optional[float] = None, client_options: Optional[Dict] = None, api_transport: Optional[str] = None, additional_headers: Optional[Dict[str, str]] = None, response_modalities: Optional[List[google.ai.generativelanguage_v1beta.types.generative_service.GenerationConfig.Modality]] = None, thinking_budget: Optional[int] = None, include_thoughts: Optional[bool] = None, safety_settings: Optional[Dict[google.ai.generativelanguage_v1beta.types.safety.HarmCategory, google.ai.generativelanguage_v1beta.types.safety.SafetySetting.HarmBlockThreshold]] = None, client: Any = None, async_client_running: Any = None, default_metadata_input: Optional[collections.abc.Sequence[Tuple[str, str]]] = None, convert_system_message_to_human: bool = False, response_mime_type: Optional[str] = None, response_schema: Optional[Dict[str, Any]] = None, cached_content: Optional[str] = None, stop: Optional[List[str]] = None, streaming: Optional[bool] = None, model_kwargs: dict[str, typing.Any] = <factory>) -> None\n",
            " |\n",
            " |  `Google AI` chat models integration.\n",
            " |\n",
            " |  Instantiation:\n",
            " |      To use, you must have either:\n",
            " |\n",
            " |          1. The ``GOOGLE_API_KEY`` environment variable set with your API key, or\n",
            " |          2. Pass your API key using the ``google_api_key`` kwarg to the\n",
            " |          ChatGoogleGenerativeAI constructor.\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |\n",
            " |          llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
            " |          llm.invoke(\"Write me a ballad about LangChain\")\n",
            " |\n",
            " |  Invoke:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          messages = [\n",
            " |              (\"system\", \"Translate the user sentence to French.\"),\n",
            " |              (\"human\", \"I love programming.\"),\n",
            " |          ]\n",
            " |          llm.invoke(messages)\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          AIMessage(\n",
            " |              content=\"J'adore programmer. \\\\n\",\n",
            " |              response_metadata={\n",
            " |                  \"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []},\n",
            " |                  \"finish_reason\": \"STOP\",\n",
            " |                  \"safety_ratings\": [\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                  ],\n",
            " |              },\n",
            " |              id=\"run-56cecc34-2e54-4b52-a974-337e47008ad2-0\",\n",
            " |              usage_metadata={\n",
            " |                  \"input_tokens\": 18,\n",
            " |                  \"output_tokens\": 5,\n",
            " |                  \"total_tokens\": 23,\n",
            " |              },\n",
            " |          )\n",
            " |\n",
            " |  Stream:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          for chunk in llm.stream(messages):\n",
            " |              print(chunk)\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          AIMessageChunk(\n",
            " |              content=\"J\",\n",
            " |              response_metadata={\"finish_reason\": \"STOP\", \"safety_ratings\": []},\n",
            " |              id=\"run-e905f4f4-58cb-4a10-a960-448a2bb649e3\",\n",
            " |              usage_metadata={\n",
            " |                  \"input_tokens\": 18,\n",
            " |                  \"output_tokens\": 1,\n",
            " |                  \"total_tokens\": 19,\n",
            " |              },\n",
            " |          )\n",
            " |          AIMessageChunk(\n",
            " |              content=\"'adore programmer. \\\\n\",\n",
            " |              response_metadata={\n",
            " |                  \"finish_reason\": \"STOP\",\n",
            " |                  \"safety_ratings\": [\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                  ],\n",
            " |              },\n",
            " |              id=\"run-e905f4f4-58cb-4a10-a960-448a2bb649e3\",\n",
            " |              usage_metadata={\n",
            " |                  \"input_tokens\": 18,\n",
            " |                  \"output_tokens\": 5,\n",
            " |                  \"total_tokens\": 23,\n",
            " |              },\n",
            " |          )\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          stream = llm.stream(messages)\n",
            " |          full = next(stream)\n",
            " |          for chunk in stream:\n",
            " |              full += chunk\n",
            " |          full\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          AIMessageChunk(\n",
            " |              content=\"J'adore programmer. \\\\n\",\n",
            " |              response_metadata={\n",
            " |                  \"finish_reason\": \"STOPSTOP\",\n",
            " |                  \"safety_ratings\": [\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                      {\n",
            " |                          \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            " |                          \"probability\": \"NEGLIGIBLE\",\n",
            " |                          \"blocked\": False,\n",
            " |                      },\n",
            " |                  ],\n",
            " |              },\n",
            " |              id=\"run-3ce13a42-cd30-4ad7-a684-f1f0b37cdeec\",\n",
            " |              usage_metadata={\n",
            " |                  \"input_tokens\": 36,\n",
            " |                  \"output_tokens\": 6,\n",
            " |                  \"total_tokens\": 42,\n",
            " |              },\n",
            " |          )\n",
            " |\n",
            " |  Async:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          await llm.ainvoke(messages)\n",
            " |\n",
            " |          # stream:\n",
            " |          # async for chunk in (await llm.astream(messages))\n",
            " |\n",
            " |          # batch:\n",
            " |          # await llm.abatch([messages])\n",
            " |\n",
            " |  Context Caching:\n",
            " |      Context caching allows you to store and reuse content (e.g., PDFs, images) for\n",
            " |      faster processing. The ``cached_content`` parameter accepts a cache name created\n",
            " |      via the Google Generative AI API. Below are two examples: caching a single file\n",
            " |      directly and caching multiple files using ``Part``.\n",
            " |\n",
            " |      Single File Example:\n",
            " |      This caches a single file and queries it.\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from google import genai\n",
            " |          from google.genai import types\n",
            " |          import time\n",
            " |          from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |          from langchain_core.messages import HumanMessage\n",
            " |\n",
            " |          client = genai.Client()\n",
            " |\n",
            " |          # Upload file\n",
            " |          file = client.files.upload(file=\"./example_file\")\n",
            " |          while file.state.name == \"PROCESSING\":\n",
            " |              time.sleep(2)\n",
            " |              file = client.files.get(name=file.name)\n",
            " |\n",
            " |          # Create cache\n",
            " |          model = \"models/gemini-1.5-flash-latest\"\n",
            " |          cache = client.caches.create(\n",
            " |              model=model,\n",
            " |              config=types.CreateCachedContentConfig(\n",
            " |                  display_name=\"Cached Content\",\n",
            " |                  system_instruction=(\n",
            " |                      \"You are an expert content analyzer, and your job is to answer \"\n",
            " |                      \"the user's query based on the file you have access to.\"\n",
            " |                  ),\n",
            " |                  contents=[file],\n",
            " |                  ttl=\"300s\",\n",
            " |              ),\n",
            " |          )\n",
            " |\n",
            " |          # Query with LangChain\n",
            " |          llm = ChatGoogleGenerativeAI(\n",
            " |              model=model,\n",
            " |              cached_content=cache.name,\n",
            " |          )\n",
            " |          message = HumanMessage(content=\"Summarize the main points of the content.\")\n",
            " |          llm.invoke([message])\n",
            " |\n",
            " |      Multiple Files Example:\n",
            " |      This caches two files using `Part` and queries them together.\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from google import genai\n",
            " |          from google.genai.types import CreateCachedContentConfig, Content, Part\n",
            " |          import time\n",
            " |          from langchain_google_genai import ChatGoogleGenerativeAI\n",
            " |          from langchain_core.messages import HumanMessage\n",
            " |\n",
            " |          client = genai.Client()\n",
            " |\n",
            " |          # Upload files\n",
            " |          file_1 = client.files.upload(file=\"./file1\")\n",
            " |          while file_1.state.name == \"PROCESSING\":\n",
            " |              time.sleep(2)\n",
            " |              file_1 = client.files.get(name=file_1.name)\n",
            " |\n",
            " |          file_2 = client.files.upload(file=\"./file2\")\n",
            " |          while file_2.state.name == \"PROCESSING\":\n",
            " |              time.sleep(2)\n",
            " |              file_2 = client.files.get(name=file_2.name)\n",
            " |\n",
            " |          # Create cache with multiple files\n",
            " |          contents = [\n",
            " |              Content(\n",
            " |                  role=\"user\",\n",
            " |                  parts=[\n",
            " |                      Part.from_uri(file_uri=file_1.uri, mime_type=file_1.mime_type),\n",
            " |                      Part.from_uri(file_uri=file_2.uri, mime_type=file_2.mime_type),\n",
            " |                  ],\n",
            " |              )\n",
            " |          ]\n",
            " |          model = \"gemini-1.5-flash-latest\"\n",
            " |          cache = client.caches.create(\n",
            " |              model=model,\n",
            " |              config=CreateCachedContentConfig(\n",
            " |                  display_name=\"Cached Contents\",\n",
            " |                  system_instruction=(\n",
            " |                      \"You are an expert content analyzer, and your job is to answer \"\n",
            " |                      \"the user's query based on the files you have access to.\"\n",
            " |                  ),\n",
            " |                  contents=contents,\n",
            " |                  ttl=\"300s\",\n",
            " |              ),\n",
            " |          )\n",
            " |\n",
            " |          # Query with LangChain\n",
            " |          llm = ChatGoogleGenerativeAI(\n",
            " |              model=model,\n",
            " |              cached_content=cache.name,\n",
            " |          )\n",
            " |          message = HumanMessage(\n",
            " |              content=\"Provide a summary of the key information across both files.\"\n",
            " |          )\n",
            " |          llm.invoke([message])\n",
            " |\n",
            " |  Tool calling:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from pydantic import BaseModel, Field\n",
            " |\n",
            " |\n",
            " |          class GetWeather(BaseModel):\n",
            " |              '''Get the current weather in a given location'''\n",
            " |\n",
            " |              location: str = Field(\n",
            " |                  ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
            " |              )\n",
            " |\n",
            " |\n",
            " |          class GetPopulation(BaseModel):\n",
            " |              '''Get the current population in a given location'''\n",
            " |\n",
            " |              location: str = Field(\n",
            " |                  ..., description=\"The city and state, e.g. San Francisco, CA\"\n",
            " |              )\n",
            " |\n",
            " |\n",
            " |          llm_with_tools = llm.bind_tools([GetWeather, GetPopulation])\n",
            " |          ai_msg = llm_with_tools.invoke(\n",
            " |              \"Which city is hotter today and which is bigger: LA or NY?\"\n",
            " |          )\n",
            " |          ai_msg.tool_calls\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          [\n",
            " |              {\n",
            " |                  \"name\": \"GetWeather\",\n",
            " |                  \"args\": {\"location\": \"Los Angeles, CA\"},\n",
            " |                  \"id\": \"c186c99f-f137-4d52-947f-9e3deabba6f6\",\n",
            " |              },\n",
            " |              {\n",
            " |                  \"name\": \"GetWeather\",\n",
            " |                  \"args\": {\"location\": \"New York City, NY\"},\n",
            " |                  \"id\": \"cebd4a5d-e800-4fa5-babd-4aa286af4f31\",\n",
            " |              },\n",
            " |              {\n",
            " |                  \"name\": \"GetPopulation\",\n",
            " |                  \"args\": {\"location\": \"Los Angeles, CA\"},\n",
            " |                  \"id\": \"4f92d897-f5e4-4d34-a3bc-93062c92591e\",\n",
            " |              },\n",
            " |              {\n",
            " |                  \"name\": \"GetPopulation\",\n",
            " |                  \"args\": {\"location\": \"New York City, NY\"},\n",
            " |                  \"id\": \"634582de-5186-4e4b-968b-f192f0a93678\",\n",
            " |              },\n",
            " |          ]\n",
            " |\n",
            " |  Use Search with Gemini 2:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from google.ai.generativelanguage_v1beta.types import Tool as GenAITool\n",
            " |\n",
            " |          llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
            " |          resp = llm.invoke(\n",
            " |              \"When is the next total solar eclipse in US?\",\n",
            " |              tools=[GenAITool(google_search={})],\n",
            " |          )\n",
            " |\n",
            " |  Structured output:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from typing import Optional\n",
            " |\n",
            " |          from pydantic import BaseModel, Field\n",
            " |\n",
            " |\n",
            " |          class Joke(BaseModel):\n",
            " |              '''Joke to tell user.'''\n",
            " |\n",
            " |              setup: str = Field(description=\"The setup of the joke\")\n",
            " |              punchline: str = Field(description=\"The punchline to the joke\")\n",
            " |              rating: Optional[int] = Field(\n",
            " |                  description=\"How funny the joke is, from 1 to 10\"\n",
            " |              )\n",
            " |\n",
            " |\n",
            " |          structured_llm = llm.with_structured_output(Joke)\n",
            " |          structured_llm.invoke(\"Tell me a joke about cats\")\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          Joke(\n",
            " |              setup=\"Why are cats so good at video games?\",\n",
            " |              punchline=\"They have nine lives on the internet\",\n",
            " |              rating=None,\n",
            " |          )\n",
            " |\n",
            " |  Image input:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          import base64\n",
            " |          import httpx\n",
            " |          from langchain_core.messages import HumanMessage\n",
            " |\n",
            " |          image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
            " |          image_data = base64.b64encode(httpx.get(image_url).content).decode(\"utf-8\")\n",
            " |          message = HumanMessage(\n",
            " |              content=[\n",
            " |                  {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
            " |                  {\n",
            " |                      \"type\": \"image_url\",\n",
            " |                      \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_data}\"},\n",
            " |                  },\n",
            " |              ]\n",
            " |          )\n",
            " |          ai_msg = llm.invoke([message])\n",
            " |          ai_msg.content\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          \"The weather in this image appears to be sunny and pleasant. The sky is a\n",
            " |          bright blue with scattered white clouds, suggesting fair weather. The lush\n",
            " |          green grass and trees indicate a warm and possibly slightly breezy day.\n",
            " |          There are no signs of rain or storms.\"\n",
            " |\n",
            " |  PDF input:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          import base64\n",
            " |          from langchain_core.messages import HumanMessage\n",
            " |\n",
            " |          pdf_bytes = open(\"/path/to/your/test.pdf\", \"rb\").read()\n",
            " |          pdf_base64 = base64.b64encode(pdf_bytes).decode(\"utf-8\")\n",
            " |\n",
            " |          message = HumanMessage(\n",
            " |              content=[\n",
            " |                  {\"type\": \"text\", \"text\": \"describe the document in a sentence\"},\n",
            " |                  {\n",
            " |                      \"type\": \"file\",\n",
            " |                      \"source_type\": \"base64\",\n",
            " |                      \"mime_type\": \"application/pdf\",\n",
            " |                      \"data\": pdf_base64,\n",
            " |                  },\n",
            " |              ]\n",
            " |          )\n",
            " |          ai_msg = llm.invoke([message])\n",
            " |          ai_msg.content\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          \"This research paper describes a system developed for SemEval-2025 Task 9,\n",
            " |          which aims to automate the detection of food hazards from recall reports,\n",
            " |          addressing the class imbalance problem by leveraging LLM-based data\n",
            " |          augmentation techniques and transformer-based models to improve\n",
            " |          performance.\"\n",
            " |\n",
            " |  Video input:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          import base64\n",
            " |          from langchain_core.messages import HumanMessage\n",
            " |\n",
            " |          video_bytes = open(\"/path/to/your/video.mp4\", \"rb\").read()\n",
            " |          video_base64 = base64.b64encode(video_bytes).decode(\"utf-8\")\n",
            " |\n",
            " |          message = HumanMessage(\n",
            " |              content=[\n",
            " |                  {\n",
            " |                      \"type\": \"text\",\n",
            " |                      \"text\": \"describe what's in this video in a sentence\",\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"type\": \"file\",\n",
            " |                      \"source_type\": \"base64\",\n",
            " |                      \"mime_type\": \"video/mp4\",\n",
            " |                      \"data\": video_base64,\n",
            " |                  },\n",
            " |              ]\n",
            " |          )\n",
            " |          ai_msg = llm.invoke([message])\n",
            " |          ai_msg.content\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          \"Tom and Jerry, along with a turkey, engage in a chaotic Thanksgiving-themed\n",
            " |          adventure involving a corn-on-the-cob chase, maze antics, and a disastrous\n",
            " |          attempt to prepare a turkey dinner.\"\n",
            " |\n",
            " |      You can also pass YouTube URLs directly:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_core.messages import HumanMessage\n",
            " |\n",
            " |          message = HumanMessage(\n",
            " |              content=[\n",
            " |                  {\"type\": \"text\", \"text\": \"summarize the video in 3 sentences.\"},\n",
            " |                  {\n",
            " |                      \"type\": \"media\",\n",
            " |                      \"file_uri\": \"https://www.youtube.com/watch?v=9hE5-98ZeCg\",\n",
            " |                      \"mime_type\": \"video/mp4\",\n",
            " |                  },\n",
            " |              ]\n",
            " |          )\n",
            " |          ai_msg = llm.invoke([message])\n",
            " |          ai_msg.content\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          \"The video is a demo of multimodal live streaming in Gemini 2.0. The\n",
            " |          narrator is sharing his screen in AI Studio and asks if the AI can see it.\n",
            " |          The AI then reads text that is highlighted on the screen, defines the word\n",
            " |          multimodal, and summarizes everything that was seen and heard.\"\n",
            " |\n",
            " |  Audio input:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          import base64\n",
            " |          from langchain_core.messages import HumanMessage\n",
            " |\n",
            " |          audio_bytes = open(\"/path/to/your/audio.mp3\", \"rb\").read()\n",
            " |          audio_base64 = base64.b64encode(audio_bytes).decode(\"utf-8\")\n",
            " |\n",
            " |          message = HumanMessage(\n",
            " |              content=[\n",
            " |                  {\"type\": \"text\", \"text\": \"summarize this audio in a sentence\"},\n",
            " |                  {\n",
            " |                      \"type\": \"file\",\n",
            " |                      \"source_type\": \"base64\",\n",
            " |                      \"mime_type\": \"audio/mp3\",\n",
            " |                      \"data\": audio_base64,\n",
            " |                  },\n",
            " |              ]\n",
            " |          )\n",
            " |          ai_msg = llm.invoke([message])\n",
            " |          ai_msg.content\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          \"In this episode of the Made by Google podcast, Stephen Johnson and Simon\n",
            " |          Tokumine discuss NotebookLM, a tool designed to help users understand\n",
            " |          complex material in various modalities, with a focus on its unexpected uses,\n",
            " |          the development of audio overviews, and the implementation of new features\n",
            " |          like mind maps and source discovery.\"\n",
            " |\n",
            " |  File upload (URI-based):\n",
            " |      You can also upload files to Google's servers and reference them by URI.\n",
            " |      This works for PDFs, images, videos, and audio files.\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          import time\n",
            " |          from google import genai\n",
            " |          from langchain_core.messages import HumanMessage\n",
            " |\n",
            " |          client = genai.Client()\n",
            " |\n",
            " |          myfile = client.files.upload(file=\"/path/to/your/sample.pdf\")\n",
            " |          while myfile.state.name == \"PROCESSING\":\n",
            " |              time.sleep(2)\n",
            " |              myfile = client.files.get(name=myfile.name)\n",
            " |\n",
            " |          message = HumanMessage(\n",
            " |              content=[\n",
            " |                  {\"type\": \"text\", \"text\": \"What is in the document?\"},\n",
            " |                  {\n",
            " |                      \"type\": \"media\",\n",
            " |                      \"file_uri\": myfile.uri,\n",
            " |                      \"mime_type\": \"application/pdf\",\n",
            " |                  },\n",
            " |              ]\n",
            " |          )\n",
            " |          ai_msg = llm.invoke([message])\n",
            " |          ai_msg.content\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          \"This research paper assesses and mitigates multi-turn jailbreak\n",
            " |          vulnerabilities in large language models using the Crescendo attack study,\n",
            " |          evaluating attack success rates and mitigation strategies like prompt\n",
            " |          hardening and LLM-as-guardrail.\"\n",
            " |\n",
            " |  Token usage:\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          ai_msg = llm.invoke(messages)\n",
            " |          ai_msg.usage_metadata\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          {\"input_tokens\": 18, \"output_tokens\": 5, \"total_tokens\": 23}\n",
            " |\n",
            " |\n",
            " |  Response metadata\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          ai_msg = llm.invoke(messages)\n",
            " |          ai_msg.response_metadata\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          {\n",
            " |              \"prompt_feedback\": {\"block_reason\": 0, \"safety_ratings\": []},\n",
            " |              \"finish_reason\": \"STOP\",\n",
            " |              \"safety_ratings\": [\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_HATE_SPEECH\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_HARASSMENT\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |                  {\n",
            " |                      \"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
            " |                      \"probability\": \"NEGLIGIBLE\",\n",
            " |                      \"blocked\": False,\n",
            " |                  },\n",
            " |              ],\n",
            " |          }\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      ChatGoogleGenerativeAI\n",
            " |      langchain_google_genai._common._BaseGoogleGenerativeAI\n",
            " |      langchain_core.language_models.chat_models.BaseChatModel\n",
            " |      langchain_core.language_models.base.BaseLanguageModel[BaseMessage]\n",
            " |      langchain_core.language_models.base.BaseLanguageModel\n",
            " |      langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], TypeVar]\n",
            " |      langchain_core.runnables.base.RunnableSerializable\n",
            " |      langchain_core.load.serializable.Serializable\n",
            " |      pydantic.main.BaseModel\n",
            " |      langchain_core.runnables.base.Runnable\n",
            " |      abc.ABC\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, **kwargs: 'Any') -> 'None'\n",
            " |      Needed for arg validation.\n",
            " |\n",
            " |  bind_tools(self, tools: 'Sequence[dict[str, Any] | type | Callable[..., Any] | BaseTool | GoogleTool]', tool_config: 'Optional[Union[Dict, _ToolConfigDict]]' = None, *, tool_choice: 'Optional[Union[_ToolChoiceType, bool]]' = None, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, BaseMessage]'\n",
            " |      Bind tool-like objects to this chat model.\n",
            " |\n",
            " |      Assumes model is compatible with google-generativeAI tool-calling API.\n",
            " |\n",
            " |      Args:\n",
            " |          tools: A list of tool definitions to bind to this chat model.\n",
            " |              Can be a pydantic model, callable, or BaseTool. Pydantic models,\n",
            " |              callables, and BaseTools will be automatically converted to their schema\n",
            " |              dictionary representation. Tools with Union types in their arguments are\n",
            " |              now supported and converted to `anyOf` schemas.\n",
            " |          **kwargs: Any additional parameters to pass to the\n",
            " |              :class:`~langchain.runnable.Runnable` constructor.\n",
            " |\n",
            " |  get_num_tokens(self, text: 'str') -> 'int'\n",
            " |      Get the number of tokens present in the text.\n",
            " |\n",
            " |      Useful for checking if an input will fit in a model's context window.\n",
            " |\n",
            " |      Args:\n",
            " |          text: The string input to tokenize.\n",
            " |\n",
            " |      Returns:\n",
            " |          The integer number of tokens in the text.\n",
            " |\n",
            " |  invoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, code_execution: 'Optional[bool]' = None, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      Enable code execution. Supported on: gemini-1.5-pro, gemini-1.5-flash,\n",
            " |      gemini-2.0-flash, and gemini-2.0-pro. When enabled, the model can execute\n",
            " |      code to solve problems.\n",
            " |\n",
            " |  validate_environment(self) -> 'Self'\n",
            " |      Validates params and passes them to google-generativeai package.\n",
            " |\n",
            " |  with_structured_output(self, schema: 'Union[Dict, Type[BaseModel]]', method: \"Optional[Literal['function_calling', 'json_mode']]\" = 'function_calling', *, include_raw: 'bool' = False, **kwargs: 'Any') -> 'Runnable[LanguageModelInput, Union[Dict, BaseModel]]'\n",
            " |      Model wrapper that returns outputs formatted to match the given schema.\n",
            " |\n",
            " |      Args:\n",
            " |          schema: The output schema. Can be passed in as:\n",
            " |\n",
            " |              - an OpenAI function/tool schema,\n",
            " |              - a JSON Schema,\n",
            " |              - a TypedDict class,\n",
            " |              - or a Pydantic class.\n",
            " |\n",
            " |              If ``schema`` is a Pydantic class then the model output will be a\n",
            " |              Pydantic instance of that class, and the model-generated fields will be\n",
            " |              validated by the Pydantic class. Otherwise the model output will be a\n",
            " |              dict and will not be validated. See :meth:`langchain_core.utils.function_calling.convert_to_openai_tool`\n",
            " |              for more on how to properly specify types and descriptions of\n",
            " |              schema fields when specifying a Pydantic or TypedDict class.\n",
            " |\n",
            " |          include_raw:\n",
            " |              If False then only the parsed structured output is returned. If\n",
            " |              an error occurs during model output parsing it will be raised. If True\n",
            " |              then both the raw model response (a BaseMessage) and the parsed model\n",
            " |              response will be returned. If an error occurs during output parsing it\n",
            " |              will be caught and returned as well. The final output is always a dict\n",
            " |              with keys ``'raw'``, ``'parsed'``, and ``'parsing_error'``.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValueError: If there are any unsupported ``kwargs``.\n",
            " |          NotImplementedError: If the model does not implement\n",
            " |              ``with_structured_output()``.\n",
            " |\n",
            " |      Returns:\n",
            " |          A Runnable that takes same inputs as a :class:`langchain_core.language_models.chat.BaseChatModel`.\n",
            " |\n",
            " |          If ``include_raw`` is False and ``schema`` is a Pydantic class, Runnable outputs\n",
            " |          an instance of ``schema`` (i.e., a Pydantic object).\n",
            " |\n",
            " |          Otherwise, if ``include_raw`` is False then Runnable outputs a dict.\n",
            " |\n",
            " |          If ``include_raw`` is True, then Runnable outputs a dict with keys:\n",
            " |\n",
            " |          - ``'raw'``: BaseMessage\n",
            " |          - ``'parsed'``: None if there was a parsing error, otherwise the type depends on the ``schema`` as described above.\n",
            " |          - ``'parsing_error'``: Optional[BaseException]\n",
            " |\n",
            " |      Example: Pydantic schema (include_raw=False):\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |              from pydantic import BaseModel\n",
            " |\n",
            " |\n",
            " |              class AnswerWithJustification(BaseModel):\n",
            " |                  '''An answer to the user question along with justification for the answer.'''\n",
            " |\n",
            " |                  answer: str\n",
            " |                  justification: str\n",
            " |\n",
            " |\n",
            " |              llm = ChatModel(model=\"model-name\", temperature=0)\n",
            " |              structured_llm = llm.with_structured_output(AnswerWithJustification)\n",
            " |\n",
            " |              structured_llm.invoke(\n",
            " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
            " |              )\n",
            " |\n",
            " |              # -> AnswerWithJustification(\n",
            " |              #     answer='They weigh the same',\n",
            " |              #     justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'\n",
            " |              # )\n",
            " |\n",
            " |      Example: Pydantic schema (include_raw=True):\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |              from pydantic import BaseModel\n",
            " |\n",
            " |\n",
            " |              class AnswerWithJustification(BaseModel):\n",
            " |                  '''An answer to the user question along with justification for the answer.'''\n",
            " |\n",
            " |                  answer: str\n",
            " |                  justification: str\n",
            " |\n",
            " |\n",
            " |              llm = ChatModel(model=\"model-name\", temperature=0)\n",
            " |              structured_llm = llm.with_structured_output(\n",
            " |                  AnswerWithJustification, include_raw=True\n",
            " |              )\n",
            " |\n",
            " |              structured_llm.invoke(\n",
            " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
            " |              )\n",
            " |              # -> {\n",
            " |              #     'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_Ao02pnFYXD6GN1yzc0uXPsvF', 'function': {'arguments': '{\"answer\":\"They weigh the same.\",\"justification\":\"Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.\"}', 'name': 'AnswerWithJustification'}, 'type': 'function'}]}),\n",
            " |              #     'parsed': AnswerWithJustification(answer='They weigh the same.', justification='Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume or density of the objects may differ.'),\n",
            " |              #     'parsing_error': None\n",
            " |              # }\n",
            " |\n",
            " |      Example: Dict schema (include_raw=False):\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |              from pydantic import BaseModel\n",
            " |              from langchain_core.utils.function_calling import convert_to_openai_tool\n",
            " |\n",
            " |\n",
            " |              class AnswerWithJustification(BaseModel):\n",
            " |                  '''An answer to the user question along with justification for the answer.'''\n",
            " |\n",
            " |                  answer: str\n",
            " |                  justification: str\n",
            " |\n",
            " |\n",
            " |              dict_schema = convert_to_openai_tool(AnswerWithJustification)\n",
            " |              llm = ChatModel(model=\"model-name\", temperature=0)\n",
            " |              structured_llm = llm.with_structured_output(dict_schema)\n",
            " |\n",
            " |              structured_llm.invoke(\n",
            " |                  \"What weighs more a pound of bricks or a pound of feathers\"\n",
            " |              )\n",
            " |              # -> {\n",
            " |              #     'answer': 'They weigh the same',\n",
            " |              #     'justification': 'Both a pound of bricks and a pound of feathers weigh one pound. The weight is the same, but the volume and density of the two substances differ.'\n",
            " |              # }\n",
            " |\n",
            " |      .. versionchanged:: 0.2.26\n",
            " |\n",
            " |              Added support for TypedDict class.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |\n",
            " |  build_extra(values: 'dict[str, Any]') -> 'Any'\n",
            " |      Build extra kwargs from additional params that were passed in.\n",
            " |\n",
            " |  is_lc_serializable() -> 'bool'\n",
            " |      Is this class serializable?\n",
            " |\n",
            " |      By design, even if a class inherits from Serializable, it is not serializable by\n",
            " |      default. This is to prevent accidental serialization of objects that should not\n",
            " |      be serialized.\n",
            " |\n",
            " |      Returns:\n",
            " |          Whether the class is serializable. Default is False.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |\n",
            " |  async_client\n",
            " |\n",
            " |  lc_secrets\n",
            " |      A map of constructor argument names to secret ids.\n",
            " |\n",
            " |      For example,\n",
            " |          {\"openai_api_key\": \"OPENAI_API_KEY\"}\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __abstractmethods__ = frozenset()\n",
            " |\n",
            " |  __annotations__ = {'async_client_running': 'Any', 'cached_content': 'O...\n",
            " |\n",
            " |  __class_vars__ = set()\n",
            " |\n",
            " |  __parameters__ = ()\n",
            " |\n",
            " |  __private_attributes__ = {}\n",
            " |\n",
            " |  __pydantic_complete__ = True\n",
            " |\n",
            " |  __pydantic_computed_fields__ = {}\n",
            " |\n",
            " |  __pydantic_core_schema__ = {'function': {'function': <function ChatGoo...\n",
            " |\n",
            " |  __pydantic_custom_init__ = True\n",
            " |\n",
            " |  __pydantic_decorators__ = DecoratorInfos(validators={}, field_validato...\n",
            " |\n",
            " |  __pydantic_fields__ = {'additional_headers': FieldInfo(annotation=Unio...\n",
            " |\n",
            " |  __pydantic_generic_metadata__ = {'args': (), 'origin': None, 'paramete...\n",
            " |\n",
            " |  __pydantic_parent_namespace__ = None\n",
            " |\n",
            " |  __pydantic_post_init__ = None\n",
            " |\n",
            " |  __pydantic_serializer__ = SchemaSerializer(serializer=Model(\n",
            " |      Model...\n",
            " |\n",
            " |  __pydantic_setattr_handlers__ = {}\n",
            " |\n",
            " |  __pydantic_validator__ = SchemaValidator(title=\"ChatGoogleGenerativeAI...\n",
            " |\n",
            " |  __signature__ = <Signature (*, name: Optional[str] = None, cache...arg...\n",
            " |\n",
            " |  model_config = {'arbitrary_types_allowed': True, 'extra': 'ignore', 'p...\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from langchain_google_genai._common._BaseGoogleGenerativeAI:\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |\n",
            " |  __call__(self, messages: 'list[BaseMessage]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
            " |\n",
            " |      Call the model.\n",
            " |\n",
            " |      Args:\n",
            " |          messages: List of messages.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValueError: If the generation is not a chat generation.\n",
            " |\n",
            " |      Returns:\n",
            " |          The model output message.\n",
            " |\n",
            " |  async agenerate(self, messages: 'list[list[BaseMessage]]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Asynchronously pass a sequence of prompts to a model and return generations.\n",
            " |\n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |\n",
            " |      Use this method when you want to:\n",
            " |\n",
            " |      1. Take advantage of batched calls,\n",
            " |      2. Need more output from the model than just the top generated value,\n",
            " |      3. Are building chains that are agnostic to the underlying language model\n",
            " |         type (e.g., pure text completion models vs chat models).\n",
            " |\n",
            " |      Args:\n",
            " |          messages: List of list of messages.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          tags: The tags to apply.\n",
            " |          metadata: The metadata to apply.\n",
            " |          run_name: The name of the run.\n",
            " |          run_id: The ID of the run.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |\n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |          prompt and additional model provider-specific output.\n",
            " |\n",
            " |  async agenerate_prompt(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Asynchronously pass a sequence of prompts and return model generations.\n",
            " |\n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |\n",
            " |      Use this method when you want to:\n",
            " |\n",
            " |      1. Take advantage of batched calls,\n",
            " |      2. Need more output from the model than just the top generated value,\n",
            " |      3. Are building chains that are agnostic to the underlying language model\n",
            " |         type (e.g., pure text completion models vs chat models).\n",
            " |\n",
            " |      Args:\n",
            " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
            " |              converted to match the format of any language model (string for pure\n",
            " |              text generation models and BaseMessages for chat models).\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |\n",
            " |      Returns:\n",
            " |          An ``LLMResult``, which contains a list of candidate Generations for each\n",
            " |          input prompt and additional model provider-specific output.\n",
            " |\n",
            " |  async ainvoke(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      Transform a single input into an output.\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the ``Runnable``.\n",
            " |          config: A config to use when invoking the ``Runnable``.\n",
            " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
            " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
            " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
            " |              for more details. Defaults to None.\n",
            " |\n",
            " |      Returns:\n",
            " |          The output of the ``Runnable``.\n",
            " |\n",
            " |  async apredict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      .. deprecated:: 0.1.7 Use :meth:`~ainvoke` instead. It will not be removed until langchain-core==1.0.\n",
            " |\n",
            " |  async apredict_messages(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      .. deprecated:: 0.1.7 Use :meth:`~ainvoke` instead. It will not be removed until langchain-core==1.0.\n",
            " |\n",
            " |  async astream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[BaseMessageChunk]'\n",
            " |      Default implementation of ``astream``, which calls ``ainvoke``.\n",
            " |\n",
            " |      Subclasses should override this method if they support streaming output.\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the ``Runnable``.\n",
            " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
            " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Yields:\n",
            " |          The output of the ``Runnable``.\n",
            " |\n",
            " |  call_as_llm(self, message: 'str', stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
            " |\n",
            " |      Call the model.\n",
            " |\n",
            " |      Args:\n",
            " |          message: The input message.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |\n",
            " |      Returns:\n",
            " |          The model output string.\n",
            " |\n",
            " |  dict(self, **kwargs: 'Any') -> 'dict'\n",
            " |      Return a dictionary of the LLM.\n",
            " |\n",
            " |  generate(self, messages: 'list[list[BaseMessage]]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, *, tags: 'Optional[list[str]]' = None, metadata: 'Optional[dict[str, Any]]' = None, run_name: 'Optional[str]' = None, run_id: 'Optional[uuid.UUID]' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Pass a sequence of prompts to the model and return model generations.\n",
            " |\n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |\n",
            " |      Use this method when you want to:\n",
            " |\n",
            " |      1. Take advantage of batched calls,\n",
            " |      2. Need more output from the model than just the top generated value,\n",
            " |      3. Are building chains that are agnostic to the underlying language model\n",
            " |         type (e.g., pure text completion models vs chat models).\n",
            " |\n",
            " |      Args:\n",
            " |          messages: List of list of messages.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          tags: The tags to apply.\n",
            " |          metadata: The metadata to apply.\n",
            " |          run_name: The name of the run.\n",
            " |          run_id: The ID of the run.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |\n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |          prompt and additional model provider-specific output.\n",
            " |\n",
            " |  generate_prompt(self, prompts: 'list[PromptValue]', stop: 'Optional[list[str]]' = None, callbacks: 'Callbacks' = None, **kwargs: 'Any') -> 'LLMResult'\n",
            " |      Pass a sequence of prompts to the model and return model generations.\n",
            " |\n",
            " |      This method should make use of batched calls for models that expose a batched\n",
            " |      API.\n",
            " |\n",
            " |      Use this method when you want to:\n",
            " |\n",
            " |      1. Take advantage of batched calls,\n",
            " |      2. Need more output from the model than just the top generated value,\n",
            " |      3. Are building chains that are agnostic to the underlying language model\n",
            " |         type (e.g., pure text completion models vs chat models).\n",
            " |\n",
            " |      Args:\n",
            " |          prompts: List of PromptValues. A PromptValue is an object that can be\n",
            " |              converted to match the format of any language model (string for pure\n",
            " |              text generation models and BaseMessages for chat models).\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          callbacks: Callbacks to pass through. Used for executing additional\n",
            " |              functionality, such as logging or streaming, throughout generation.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |\n",
            " |      Returns:\n",
            " |          An LLMResult, which contains a list of candidate Generations for each input\n",
            " |          prompt and additional model provider-specific output.\n",
            " |\n",
            " |  predict(self, text: 'str', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'str'\n",
            " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
            " |\n",
            " |      Predict the next message.\n",
            " |\n",
            " |      Args:\n",
            " |          text: The input message.\n",
            " |          stop: Stop words to use when generating. Model output is cut off at the\n",
            " |              first occurrence of any of these substrings.\n",
            " |          **kwargs: Arbitrary additional keyword arguments. These are usually passed\n",
            " |              to the model provider API call.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValueError: If the output is not a string.\n",
            " |\n",
            " |      Returns:\n",
            " |          The predicted output string.\n",
            " |\n",
            " |  predict_messages(self, messages: 'list[BaseMessage]', *, stop: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'BaseMessage'\n",
            " |      .. deprecated:: 0.1.7 Use :meth:`~invoke` instead. It will not be removed until langchain-core==1.0.\n",
            " |\n",
            " |  stream(self, input: 'LanguageModelInput', config: 'Optional[RunnableConfig]' = None, *, stop: 'Optional[list[str]]' = None, **kwargs: 'Any') -> 'Iterator[BaseMessageChunk]'\n",
            " |      Default implementation of ``stream``, which calls ``invoke``.\n",
            " |\n",
            " |      Subclasses should override this method if they support streaming output.\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the ``Runnable``.\n",
            " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
            " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Yields:\n",
            " |          The output of the ``Runnable``.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |\n",
            " |  raise_deprecation(values: 'dict') -> 'Any'\n",
            " |      Emit deprecation warning if ``callback_manager`` is used.\n",
            " |\n",
            " |      Args:\n",
            " |          values (Dict): Values to validate.\n",
            " |\n",
            " |      Returns:\n",
            " |          Dict: Validated values.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.language_models.chat_models.BaseChatModel:\n",
            " |\n",
            " |  OutputType\n",
            " |      Get the output type for this runnable.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |\n",
            " |  get_num_tokens_from_messages(self, messages: 'list[BaseMessage]', tools: 'Optional[Sequence]' = None) -> 'int'\n",
            " |      Get the number of tokens in the messages.\n",
            " |\n",
            " |      Useful for checking if an input fits in a model's context window.\n",
            " |\n",
            " |      .. note::\n",
            " |          The base implementation of ``get_num_tokens_from_messages`` ignores tool\n",
            " |          schemas.\n",
            " |\n",
            " |      Args:\n",
            " |          messages: The message inputs to tokenize.\n",
            " |          tools: If provided, sequence of dict, ``BaseModel``, function, or\n",
            " |              ``BaseTools`` to be converted to tool schemas.\n",
            " |\n",
            " |      Returns:\n",
            " |          The sum of the number of tokens across the messages.\n",
            " |\n",
            " |  get_token_ids(self, text: 'str') -> 'list[int]'\n",
            " |      Return the ordered ids of the tokens in a text.\n",
            " |\n",
            " |      Args:\n",
            " |          text: The string input to tokenize.\n",
            " |\n",
            " |      Returns:\n",
            " |          A list of ids corresponding to the tokens in the text, in order they occur\n",
            " |          in the text.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |\n",
            " |  set_verbose(verbose: 'Optional[bool]') -> 'bool'\n",
            " |      If verbose is None, set it.\n",
            " |\n",
            " |      This allows users to pass in None as verbose to access the global setting.\n",
            " |\n",
            " |      Args:\n",
            " |          verbose: The verbosity setting to use.\n",
            " |\n",
            " |      Returns:\n",
            " |          The verbosity setting to use.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.language_models.base.BaseLanguageModel:\n",
            " |\n",
            " |  InputType\n",
            " |      Get the input type for this runnable.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.runnables.base.RunnableSerializable:\n",
            " |\n",
            " |  configurable_alternatives(self, which: 'ConfigurableField', *, default_key: 'str' = 'default', prefix_keys: 'bool' = False, **kwargs: 'Union[Runnable[Input, Output], Callable[[], Runnable[Input, Output]]]') -> 'RunnableSerializable[Input, Output]'\n",
            " |      Configure alternatives for ``Runnables`` that can be set at runtime.\n",
            " |\n",
            " |      Args:\n",
            " |          which: The ``ConfigurableField`` instance that will be used to select the\n",
            " |              alternative.\n",
            " |          default_key: The default key to use if no alternative is selected.\n",
            " |              Defaults to ``'default'``.\n",
            " |          prefix_keys: Whether to prefix the keys with the ``ConfigurableField`` id.\n",
            " |              Defaults to False.\n",
            " |          **kwargs: A dictionary of keys to ``Runnable`` instances or callables that\n",
            " |              return ``Runnable`` instances.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable`` with the alternatives configured.\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_anthropic import ChatAnthropic\n",
            " |          from langchain_core.runnables.utils import ConfigurableField\n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |\n",
            " |          model = ChatAnthropic(\n",
            " |              model_name=\"claude-3-7-sonnet-20250219\"\n",
            " |          ).configurable_alternatives(\n",
            " |              ConfigurableField(id=\"llm\"),\n",
            " |              default_key=\"anthropic\",\n",
            " |              openai=ChatOpenAI(),\n",
            " |          )\n",
            " |\n",
            " |          # uses the default model ChatAnthropic\n",
            " |          print(model.invoke(\"which organization created you?\").content)\n",
            " |\n",
            " |          # uses ChatOpenAI\n",
            " |          print(\n",
            " |              model.with_config(configurable={\"llm\": \"openai\"})\n",
            " |              .invoke(\"which organization created you?\")\n",
            " |              .content\n",
            " |          )\n",
            " |\n",
            " |  configurable_fields(self, **kwargs: 'AnyConfigurableField') -> 'RunnableSerializable[Input, Output]'\n",
            " |      Configure particular ``Runnable`` fields at runtime.\n",
            " |\n",
            " |      Args:\n",
            " |          **kwargs: A dictionary of ``ConfigurableField`` instances to configure.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValueError: If a configuration key is not found in the ``Runnable``.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable`` with the fields configured.\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_core.runnables import ConfigurableField\n",
            " |          from langchain_openai import ChatOpenAI\n",
            " |\n",
            " |          model = ChatOpenAI(max_tokens=20).configurable_fields(\n",
            " |              max_tokens=ConfigurableField(\n",
            " |                  id=\"output_token_number\",\n",
            " |                  name=\"Max tokens in the output\",\n",
            " |                  description=\"The maximum number of tokens in the output\",\n",
            " |              )\n",
            " |          )\n",
            " |\n",
            " |          # max_tokens = 20\n",
            " |          print(\n",
            " |              \"max_tokens_20: \", model.invoke(\"tell me something about chess\").content\n",
            " |          )\n",
            " |\n",
            " |          # max_tokens = 200\n",
            " |          print(\n",
            " |              \"max_tokens_200: \",\n",
            " |              model.with_config(configurable={\"output_token_number\": 200})\n",
            " |              .invoke(\"tell me something about chess\")\n",
            " |              .content,\n",
            " |          )\n",
            " |\n",
            " |  to_json(self) -> 'Union[SerializedConstructor, SerializedNotImplemented]'\n",
            " |      Serialize the ``Runnable`` to JSON.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON-serializable representation of the ``Runnable``.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from langchain_core.runnables.base.RunnableSerializable:\n",
            " |\n",
            " |  __orig_bases__ = (<class 'langchain_core.load.serializable.Serializabl...\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.load.serializable.Serializable:\n",
            " |\n",
            " |  __repr_args__(self) -> Any\n",
            " |\n",
            " |  to_json_not_implemented(self) -> langchain_core.load.serializable.SerializedNotImplemented\n",
            " |      Serialize a \"not implemented\" object.\n",
            " |\n",
            " |      Returns:\n",
            " |          SerializedNotImplemented.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from langchain_core.load.serializable.Serializable:\n",
            " |\n",
            " |  get_lc_namespace() -> list[str]\n",
            " |      Get the namespace of the langchain object.\n",
            " |\n",
            " |      For example, if the class is `langchain.llms.openai.OpenAI`, then the\n",
            " |      namespace is [\"langchain\", \"llms\", \"openai\"]\n",
            " |\n",
            " |      Returns:\n",
            " |          The namespace as a list of strings.\n",
            " |\n",
            " |  lc_id() -> list[str]\n",
            " |      Return a unique identifier for this class for serialization purposes.\n",
            " |\n",
            " |      The unique identifier is a list of strings that describes the path\n",
            " |      to the object.\n",
            " |      For example, for the class `langchain.llms.openai.OpenAI`, the id is\n",
            " |      [\"langchain\", \"llms\", \"openai\", \"OpenAI\"].\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.load.serializable.Serializable:\n",
            " |\n",
            " |  lc_attributes\n",
            " |      List of attribute names that should be included in the serialized kwargs.\n",
            " |\n",
            " |      These attributes must be accepted by the constructor.\n",
            " |      Default is an empty dictionary.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __copy__(self) -> 'Self'\n",
            " |      Returns a shallow copy of the model.\n",
            " |\n",
            " |  __deepcopy__(self, memo: 'dict[int, Any] | None' = None) -> 'Self'\n",
            " |      Returns a deep copy of the model.\n",
            " |\n",
            " |  __delattr__(self, item: 'str') -> 'Any'\n",
            " |      Implement delattr(self, name).\n",
            " |\n",
            " |  __eq__(self, other: 'Any') -> 'bool'\n",
            " |      Return self==value.\n",
            " |\n",
            " |  __getattr__(self, item: 'str') -> 'Any'\n",
            " |\n",
            " |  __getstate__(self) -> 'dict[Any, Any]'\n",
            " |      Helper for pickle.\n",
            " |\n",
            " |  __iter__(self) -> 'TupleGenerator'\n",
            " |      So `dict(model)` works.\n",
            " |\n",
            " |  __pretty__(self, fmt: 'typing.Callable[[Any], Any]', **kwargs: 'Any') -> 'typing.Generator[Any, None, None]' from pydantic._internal._repr.Representation\n",
            " |      Used by devtools (https://python-devtools.helpmanual.io/) to pretty print objects.\n",
            " |\n",
            " |  __replace__(self, **changes: 'Any') -> 'Self'\n",
            " |      # Because we make use of `@dataclass_transform()`, `__replace__` is already synthesized by\n",
            " |      # type checkers, so we define the implementation in this `if not TYPE_CHECKING:` block:\n",
            " |\n",
            " |  __repr__(self) -> 'str'\n",
            " |      Return repr(self).\n",
            " |\n",
            " |  __repr_name__(self) -> 'str' from pydantic._internal._repr.Representation\n",
            " |      Name of the instance's class, used in __repr__.\n",
            " |\n",
            " |  __repr_recursion__(self, object: 'Any') -> 'str' from pydantic._internal._repr.Representation\n",
            " |      Returns the string representation of a recursive object.\n",
            " |\n",
            " |  __repr_str__(self, join_str: 'str') -> 'str' from pydantic._internal._repr.Representation\n",
            " |\n",
            " |  __rich_repr__(self) -> 'RichReprResult' from pydantic._internal._repr.Representation\n",
            " |      Used by Rich (https://rich.readthedocs.io/en/stable/pretty.html) to pretty print objects.\n",
            " |\n",
            " |  __setattr__(self, name: 'str', value: 'Any') -> 'None'\n",
            " |      Implement setattr(self, name, value).\n",
            " |\n",
            " |  __setstate__(self, state: 'dict[Any, Any]') -> 'None'\n",
            " |\n",
            " |  __str__(self) -> 'str'\n",
            " |      Return str(self).\n",
            " |\n",
            " |  copy(self, *, include: 'AbstractSetIntStr | MappingIntStrAny | None' = None, exclude: 'AbstractSetIntStr | MappingIntStrAny | None' = None, update: 'Dict[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      Returns a copy of the model.\n",
            " |\n",
            " |      !!! warning \"Deprecated\"\n",
            " |          This method is now deprecated; use `model_copy` instead.\n",
            " |\n",
            " |      If you need `include` or `exclude`, use:\n",
            " |\n",
            " |      ```python {test=\"skip\" lint=\"skip\"}\n",
            " |      data = self.model_dump(include=include, exclude=exclude, round_trip=True)\n",
            " |      data = {**data, **(update or {})}\n",
            " |      copied = self.model_validate(data)\n",
            " |      ```\n",
            " |\n",
            " |      Args:\n",
            " |          include: Optional set or mapping specifying which fields to include in the copied model.\n",
            " |          exclude: Optional set or mapping specifying which fields to exclude in the copied model.\n",
            " |          update: Optional dictionary of field-value pairs to override field values in the copied model.\n",
            " |          deep: If True, the values of fields that are Pydantic models will be deep-copied.\n",
            " |\n",
            " |      Returns:\n",
            " |          A copy of the model with included, excluded and updated fields as specified.\n",
            " |\n",
            " |  json(self, *, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, by_alias: 'bool' = False, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, encoder: 'Callable[[Any], Any] | None' = PydanticUndefined, models_as_dict: 'bool' = PydanticUndefined, **dumps_kwargs: 'Any') -> 'str'\n",
            " |\n",
            " |  model_copy(self, *, update: 'Mapping[str, Any] | None' = None, deep: 'bool' = False) -> 'Self'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [`model_copy`](../concepts/serialization.md#model_copy)\n",
            " |\n",
            " |      Returns a copy of the model.\n",
            " |\n",
            " |      !!! note\n",
            " |          The underlying instance's [`__dict__`][object.__dict__] attribute is copied. This\n",
            " |          might have unexpected side effects if you store anything in it, on top of the model\n",
            " |          fields (e.g. the value of [cached properties][functools.cached_property]).\n",
            " |\n",
            " |      Args:\n",
            " |          update: Values to change/add in the new model. Note: the data is not validated\n",
            " |              before creating the new model. You should trust this data.\n",
            " |          deep: Set to `True` to make a deep copy of the model.\n",
            " |\n",
            " |      Returns:\n",
            " |          New model instance.\n",
            " |\n",
            " |  model_dump(self, *, mode: \"Literal['json', 'python'] | str\" = 'python', include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'dict[str, Any]'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [`model_dump`](../concepts/serialization.md#modelmodel_dump)\n",
            " |\n",
            " |      Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.\n",
            " |\n",
            " |      Args:\n",
            " |          mode: The mode in which `to_python` should run.\n",
            " |              If mode is 'json', the output will only contain JSON serializable types.\n",
            " |              If mode is 'python', the output may contain non-JSON-serializable Python objects.\n",
            " |          include: A set of fields to include in the output.\n",
            " |          exclude: A set of fields to exclude from the output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to use the field's alias in the dictionary key if defined.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
            " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |\n",
            " |      Returns:\n",
            " |          A dictionary representation of the model.\n",
            " |\n",
            " |  model_dump_json(self, *, indent: 'int | None' = None, include: 'IncEx | None' = None, exclude: 'IncEx | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, exclude_unset: 'bool' = False, exclude_defaults: 'bool' = False, exclude_none: 'bool' = False, round_trip: 'bool' = False, warnings: \"bool | Literal['none', 'warn', 'error']\" = True, fallback: 'Callable[[Any], Any] | None' = None, serialize_as_any: 'bool' = False) -> 'str'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [`model_dump_json`](../concepts/serialization.md#modelmodel_dump_json)\n",
            " |\n",
            " |      Generates a JSON representation of the model using Pydantic's `to_json` method.\n",
            " |\n",
            " |      Args:\n",
            " |          indent: Indentation to use in the JSON output. If None is passed, the output will be compact.\n",
            " |          include: Field(s) to include in the JSON output.\n",
            " |          exclude: Field(s) to exclude from the JSON output.\n",
            " |          context: Additional context to pass to the serializer.\n",
            " |          by_alias: Whether to serialize using field aliases.\n",
            " |          exclude_unset: Whether to exclude fields that have not been explicitly set.\n",
            " |          exclude_defaults: Whether to exclude fields that are set to their default value.\n",
            " |          exclude_none: Whether to exclude fields that have a value of `None`.\n",
            " |          round_trip: If True, dumped values should be valid as input for non-idempotent types such as Json[T].\n",
            " |          warnings: How to handle serialization errors. False/\"none\" ignores them, True/\"warn\" logs errors,\n",
            " |              \"error\" raises a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError].\n",
            " |          fallback: A function to call when an unknown value is encountered. If not provided,\n",
            " |              a [`PydanticSerializationError`][pydantic_core.PydanticSerializationError] error is raised.\n",
            " |          serialize_as_any: Whether to serialize fields with duck-typing serialization behavior.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON string representation of the model.\n",
            " |\n",
            " |  model_post_init(self, context: 'Any', /) -> 'None'\n",
            " |      Override this method to perform additional initialization after `__init__` and `model_construct`.\n",
            " |      This is useful if you want to do some validation that requires the entire model to be initialized.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __class_getitem__(typevar_values: 'type[Any] | tuple[type[Any], ...]') -> 'type[BaseModel] | _forward_ref.PydanticRecursiveRef'\n",
            " |      Parameterizes a generic class.\n",
            " |\n",
            " |      At least, parameterizing a generic class is the *main* thing this\n",
            " |      method does. For example, for some generic class `Foo`, this is called\n",
            " |      when we do `Foo[int]` - there, with `cls=Foo` and `params=int`.\n",
            " |\n",
            " |      However, note that this method is also called when defining generic\n",
            " |      classes in the first place with `class Foo[T]: ...`.\n",
            " |\n",
            " |  __get_pydantic_core_schema__(source: 'type[BaseModel]', handler: 'GetCoreSchemaHandler', /) -> 'CoreSchema'\n",
            " |\n",
            " |  __get_pydantic_json_schema__(core_schema: 'CoreSchema', handler: 'GetJsonSchemaHandler', /) -> 'JsonSchemaValue'\n",
            " |      Hook into generating the model's JSON schema.\n",
            " |\n",
            " |      Args:\n",
            " |          core_schema: A `pydantic-core` CoreSchema.\n",
            " |              You can ignore this argument and call the handler with a new CoreSchema,\n",
            " |              wrap this CoreSchema (`{'type': 'nullable', 'schema': current_schema}`),\n",
            " |              or just call the handler with the original schema.\n",
            " |          handler: Call into Pydantic's internal JSON schema generation.\n",
            " |              This will raise a `pydantic.errors.PydanticInvalidForJsonSchema` if JSON schema\n",
            " |              generation fails.\n",
            " |              Since this gets called by `BaseModel.model_json_schema` you can override the\n",
            " |              `schema_generator` argument to that function to change JSON schema generation globally\n",
            " |              for a type.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON schema, as a Python object.\n",
            " |\n",
            " |  __pydantic_init_subclass__(**kwargs: 'Any') -> 'None'\n",
            " |      This is intended to behave just like `__init_subclass__`, but is called by `ModelMetaclass`\n",
            " |      only after the class is actually fully initialized. In particular, attributes like `model_fields` will\n",
            " |      be present when this is called.\n",
            " |\n",
            " |      This is necessary because `__init_subclass__` will always be called by `type.__new__`,\n",
            " |      and it would require a prohibitively large refactor to the `ModelMetaclass` to ensure that\n",
            " |      `type.__new__` was called in such a manner that the class would already be sufficiently initialized.\n",
            " |\n",
            " |      This will receive the same `kwargs` that would be passed to the standard `__init_subclass__`, namely,\n",
            " |      any kwargs passed to the class definition that aren't used internally by pydantic.\n",
            " |\n",
            " |      Args:\n",
            " |          **kwargs: Any keyword arguments passed to the class definition that aren't used internally\n",
            " |              by pydantic.\n",
            " |\n",
            " |  construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
            " |\n",
            " |  from_orm(obj: 'Any') -> 'Self'\n",
            " |\n",
            " |  model_construct(_fields_set: 'set[str] | None' = None, **values: 'Any') -> 'Self'\n",
            " |      Creates a new instance of the `Model` class with validated data.\n",
            " |\n",
            " |      Creates a new model setting `__dict__` and `__pydantic_fields_set__` from trusted or pre-validated data.\n",
            " |      Default values are respected, but no other validation is performed.\n",
            " |\n",
            " |      !!! note\n",
            " |          `model_construct()` generally respects the `model_config.extra` setting on the provided model.\n",
            " |          That is, if `model_config.extra == 'allow'`, then all extra passed values are added to the model instance's `__dict__`\n",
            " |          and `__pydantic_extra__` fields. If `model_config.extra == 'ignore'` (the default), then all extra passed values are ignored.\n",
            " |          Because no validation is performed with a call to `model_construct()`, having `model_config.extra == 'forbid'` does not result in\n",
            " |          an error if extra values are passed, but they will be ignored.\n",
            " |\n",
            " |      Args:\n",
            " |          _fields_set: A set of field names that were originally explicitly set during instantiation. If provided,\n",
            " |              this is directly used for the [`model_fields_set`][pydantic.BaseModel.model_fields_set] attribute.\n",
            " |              Otherwise, the field names from the `values` argument will be used.\n",
            " |          values: Trusted or pre-validated data dictionary.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new instance of the `Model` class with validated data.\n",
            " |\n",
            " |  model_json_schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', schema_generator: 'type[GenerateJsonSchema]' = <class 'pydantic.json_schema.GenerateJsonSchema'>, mode: 'JsonSchemaMode' = 'validation') -> 'dict[str, Any]'\n",
            " |      Generates a JSON schema for a model class.\n",
            " |\n",
            " |      Args:\n",
            " |          by_alias: Whether to use attribute aliases or not.\n",
            " |          ref_template: The reference template.\n",
            " |          schema_generator: To override the logic used to generate the JSON schema, as a subclass of\n",
            " |              `GenerateJsonSchema` with your desired modifications\n",
            " |          mode: The mode in which to generate the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          The JSON schema for the given model class.\n",
            " |\n",
            " |  model_parametrized_name(params: 'tuple[type[Any], ...]') -> 'str'\n",
            " |      Compute the class name for parametrizations of generic classes.\n",
            " |\n",
            " |      This method can be overridden to achieve a custom naming scheme for generic BaseModels.\n",
            " |\n",
            " |      Args:\n",
            " |          params: Tuple of types of the class. Given a generic class\n",
            " |              `Model` with 2 type variables and a concrete model `Model[str, int]`,\n",
            " |              the value `(str, int)` would be passed to `params`.\n",
            " |\n",
            " |      Returns:\n",
            " |          String representing the new class where `params` are passed to `cls` as type variables.\n",
            " |\n",
            " |      Raises:\n",
            " |          TypeError: Raised when trying to generate concrete names for non-generic models.\n",
            " |\n",
            " |  model_rebuild(*, force: 'bool' = False, raise_errors: 'bool' = True, _parent_namespace_depth: 'int' = 2, _types_namespace: 'MappingNamespace | None' = None) -> 'bool | None'\n",
            " |      Try to rebuild the pydantic-core schema for the model.\n",
            " |\n",
            " |      This may be necessary when one of the annotations is a ForwardRef which could not be resolved during\n",
            " |      the initial attempt to build the schema, and automatic rebuilding fails.\n",
            " |\n",
            " |      Args:\n",
            " |          force: Whether to force the rebuilding of the model schema, defaults to `False`.\n",
            " |          raise_errors: Whether to raise errors, defaults to `True`.\n",
            " |          _parent_namespace_depth: The depth level of the parent namespace, defaults to 2.\n",
            " |          _types_namespace: The types namespace, defaults to `None`.\n",
            " |\n",
            " |      Returns:\n",
            " |          Returns `None` if the schema is already \"complete\" and rebuilding was not required.\n",
            " |          If rebuilding _was_ required, returns `True` if rebuilding was successful, otherwise `False`.\n",
            " |\n",
            " |  model_validate(obj: 'Any', *, strict: 'bool | None' = None, from_attributes: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
            " |      Validate a pydantic model instance.\n",
            " |\n",
            " |      Args:\n",
            " |          obj: The object to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          from_attributes: Whether to extract data from object attributes.\n",
            " |          context: Additional context to pass to the validator.\n",
            " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
            " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValidationError: If the object could not be validated.\n",
            " |\n",
            " |      Returns:\n",
            " |          The validated model instance.\n",
            " |\n",
            " |  model_validate_json(json_data: 'str | bytes | bytearray', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
            " |      !!! abstract \"Usage Documentation\"\n",
            " |          [JSON Parsing](../concepts/json.md#json-parsing)\n",
            " |\n",
            " |      Validate the given JSON data against the Pydantic model.\n",
            " |\n",
            " |      Args:\n",
            " |          json_data: The JSON data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
            " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
            " |\n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |\n",
            " |      Raises:\n",
            " |          ValidationError: If `json_data` is not a JSON string or the object could not be validated.\n",
            " |\n",
            " |  model_validate_strings(obj: 'Any', *, strict: 'bool | None' = None, context: 'Any | None' = None, by_alias: 'bool | None' = None, by_name: 'bool | None' = None) -> 'Self'\n",
            " |      Validate the given object with string data against the Pydantic model.\n",
            " |\n",
            " |      Args:\n",
            " |          obj: The object containing string data to validate.\n",
            " |          strict: Whether to enforce types strictly.\n",
            " |          context: Extra variables to pass to the validator.\n",
            " |          by_alias: Whether to use the field's alias when validating against the provided input data.\n",
            " |          by_name: Whether to use the field's name when validating against the provided input data.\n",
            " |\n",
            " |      Returns:\n",
            " |          The validated Pydantic model.\n",
            " |\n",
            " |  parse_file(path: 'str | Path', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
            " |\n",
            " |  parse_obj(obj: 'Any') -> 'Self'\n",
            " |\n",
            " |  parse_raw(b: 'str | bytes', *, content_type: 'str | None' = None, encoding: 'str' = 'utf8', proto: 'DeprecatedParseProtocol | None' = None, allow_pickle: 'bool' = False) -> 'Self'\n",
            " |\n",
            " |  schema(by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}') -> 'Dict[str, Any]'\n",
            " |\n",
            " |  schema_json(*, by_alias: 'bool' = True, ref_template: 'str' = '#/$defs/{model}', **dumps_kwargs: 'Any') -> 'str'\n",
            " |\n",
            " |  update_forward_refs(**localns: 'Any') -> 'None'\n",
            " |\n",
            " |  validate(value: 'Any') -> 'Self'\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __fields_set__\n",
            " |\n",
            " |  model_extra\n",
            " |      Get extra fields set during validation.\n",
            " |\n",
            " |      Returns:\n",
            " |          A dictionary of extra fields, or `None` if `config.extra` is not set to `\"allow\"`.\n",
            " |\n",
            " |  model_fields_set\n",
            " |      Returns the set of fields that have been explicitly set on this model instance.\n",
            " |\n",
            " |      Returns:\n",
            " |          A set of strings representing the fields that have been set,\n",
            " |              i.e. that were not filled from defaults.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __pydantic_extra__\n",
            " |\n",
            " |  __pydantic_fields_set__\n",
            " |\n",
            " |  __pydantic_private__\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from pydantic.main.BaseModel:\n",
            " |\n",
            " |  __hash__ = None\n",
            " |\n",
            " |  __pydantic_root_model__ = False\n",
            " |\n",
            " |  model_computed_fields = {}\n",
            " |\n",
            " |  model_fields = {'additional_headers': FieldInfo(annotation=Union[Dict[...\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from langchain_core.runnables.base.Runnable:\n",
            " |\n",
            " |  __or__(self, other: 'Union[Runnable[Any, Other], Callable[[Iterator[Any]], Iterator[Other]], Callable[[AsyncIterator[Any]], AsyncIterator[Other]], Callable[[Any], Other], Mapping[str, Union[Runnable[Any, Other], Callable[[Any], Other], Any]]]') -> 'RunnableSerializable[Input, Other]'\n",
            " |      Runnable \"or\" operator.\n",
            " |\n",
            " |      Compose this ``Runnable`` with another object to create a\n",
            " |      ``RunnableSequence``.\n",
            " |\n",
            " |      Args:\n",
            " |          other: Another ``Runnable`` or a ``Runnable``-like object.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable``.\n",
            " |\n",
            " |  __ror__(self, other: 'Union[Runnable[Other, Any], Callable[[Iterator[Other]], Iterator[Any]], Callable[[AsyncIterator[Other]], AsyncIterator[Any]], Callable[[Other], Any], Mapping[str, Union[Runnable[Other, Any], Callable[[Other], Any], Any]]]') -> 'RunnableSerializable[Other, Output]'\n",
            " |      Runnable \"reverse-or\" operator.\n",
            " |\n",
            " |      Compose this ``Runnable`` with another object to create a\n",
            " |      ``RunnableSequence``.\n",
            " |\n",
            " |      Args:\n",
            " |          other: Another ``Runnable`` or a ``Runnable``-like object.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable``.\n",
            " |\n",
            " |  async abatch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
            " |      Default implementation runs ``ainvoke`` in parallel using ``asyncio.gather``.\n",
            " |\n",
            " |      The default implementation of ``batch`` works well for IO bound runnables.\n",
            " |\n",
            " |      Subclasses should override this method if they can batch more efficiently;\n",
            " |      e.g., if the underlying ``Runnable`` uses an API which supports a batch mode.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs: A list of inputs to the ``Runnable``.\n",
            " |          config: A config to use when invoking the ``Runnable``.\n",
            " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
            " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
            " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
            " |              for more details. Defaults to None.\n",
            " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
            " |              Defaults to False.\n",
            " |          **kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Returns:\n",
            " |          A list of outputs from the ``Runnable``.\n",
            " |\n",
            " |  async abatch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'AsyncIterator[tuple[int, Union[Output, Exception]]]'\n",
            " |      Run ``ainvoke`` in parallel on a list of inputs.\n",
            " |\n",
            " |      Yields results as they complete.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs: A list of inputs to the ``Runnable``.\n",
            " |          config: A config to use when invoking the ``Runnable``.\n",
            " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
            " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
            " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
            " |              for more details. Defaults to None.\n",
            " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
            " |              Defaults to False.\n",
            " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Yields:\n",
            " |          A tuple of the index of the input and the output from the ``Runnable``.\n",
            " |\n",
            " |  as_tool(self, args_schema: 'Optional[type[BaseModel]]' = None, *, name: 'Optional[str]' = None, description: 'Optional[str]' = None, arg_types: 'Optional[dict[str, type]]' = None) -> 'BaseTool'\n",
            " |      .. beta::\n",
            " |         This API is in beta and may change in the future.\n",
            " |\n",
            " |      Create a ``BaseTool`` from a ``Runnable``.\n",
            " |\n",
            " |      ``as_tool`` will instantiate a ``BaseTool`` with a name, description, and\n",
            " |      ``args_schema`` from a ``Runnable``. Where possible, schemas are inferred\n",
            " |      from ``runnable.get_input_schema``. Alternatively (e.g., if the\n",
            " |      ``Runnable`` takes a dict as input and the specific dict keys are not typed),\n",
            " |      the schema can be specified directly with ``args_schema``. You can also\n",
            " |      pass ``arg_types`` to just specify the required arguments and their types.\n",
            " |\n",
            " |      Args:\n",
            " |          args_schema: The schema for the tool. Defaults to None.\n",
            " |          name: The name of the tool. Defaults to None.\n",
            " |          description: The description of the tool. Defaults to None.\n",
            " |          arg_types: A dictionary of argument names to types. Defaults to None.\n",
            " |\n",
            " |      Returns:\n",
            " |          A ``BaseTool`` instance.\n",
            " |\n",
            " |      Typed dict input:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from typing_extensions import TypedDict\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          class Args(TypedDict):\n",
            " |              a: int\n",
            " |              b: list[int]\n",
            " |\n",
            " |\n",
            " |          def f(x: Args) -> str:\n",
            " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(f)\n",
            " |          as_tool = runnable.as_tool()\n",
            " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
            " |\n",
            " |      ``dict`` input, specifying schema via ``args_schema``:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from typing import Any\n",
            " |          from pydantic import BaseModel, Field\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |          def f(x: dict[str, Any]) -> str:\n",
            " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
            " |\n",
            " |          class FSchema(BaseModel):\n",
            " |              \"\"\"Apply a function to an integer and list of integers.\"\"\"\n",
            " |\n",
            " |              a: int = Field(..., description=\"Integer\")\n",
            " |              b: list[int] = Field(..., description=\"List of ints\")\n",
            " |\n",
            " |          runnable = RunnableLambda(f)\n",
            " |          as_tool = runnable.as_tool(FSchema)\n",
            " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
            " |\n",
            " |      ``dict`` input, specifying schema via ``arg_types``:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from typing import Any\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          def f(x: dict[str, Any]) -> str:\n",
            " |              return str(x[\"a\"] * max(x[\"b\"]))\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(f)\n",
            " |          as_tool = runnable.as_tool(arg_types={\"a\": int, \"b\": list[int]})\n",
            " |          as_tool.invoke({\"a\": 3, \"b\": [1, 2]})\n",
            " |\n",
            " |      String input:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          def f(x: str) -> str:\n",
            " |              return x + \"a\"\n",
            " |\n",
            " |\n",
            " |          def g(x: str) -> str:\n",
            " |              return x + \"z\"\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(f) | g\n",
            " |          as_tool = runnable.as_tool()\n",
            " |          as_tool.invoke(\"b\")\n",
            " |\n",
            " |      .. versionadded:: 0.2.14\n",
            " |\n",
            " |  assign(self, **kwargs: 'Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any], Mapping[str, Union[Runnable[dict[str, Any], Any], Callable[[dict[str, Any]], Any]]]]') -> 'RunnableSerializable[Any, Any]'\n",
            " |      Assigns new fields to the dict output of this ``Runnable``.\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_community.llms.fake import FakeStreamingListLLM\n",
            " |          from langchain_core.output_parsers import StrOutputParser\n",
            " |          from langchain_core.prompts import SystemMessagePromptTemplate\n",
            " |          from langchain_core.runnables import Runnable\n",
            " |          from operator import itemgetter\n",
            " |\n",
            " |          prompt = (\n",
            " |              SystemMessagePromptTemplate.from_template(\"You are a nice assistant.\")\n",
            " |              + \"{question}\"\n",
            " |          )\n",
            " |          llm = FakeStreamingListLLM(responses=[\"foo-lish\"])\n",
            " |\n",
            " |          chain: Runnable = prompt | llm | {\"str\": StrOutputParser()}\n",
            " |\n",
            " |          chain_with_assign = chain.assign(hello=itemgetter(\"str\") | llm)\n",
            " |\n",
            " |          print(chain_with_assign.input_schema.model_json_schema())\n",
            " |          # {'title': 'PromptInput', 'type': 'object', 'properties':\n",
            " |          {'question': {'title': 'Question', 'type': 'string'}}}\n",
            " |          print(chain_with_assign.output_schema.model_json_schema())\n",
            " |          # {'title': 'RunnableSequenceOutput', 'type': 'object', 'properties':\n",
            " |          {'str': {'title': 'Str',\n",
            " |          'type': 'string'}, 'hello': {'title': 'Hello', 'type': 'string'}}}\n",
            " |\n",
            " |      Args:\n",
            " |          **kwargs: A mapping of keys to ``Runnable`` or ``Runnable``-like objects\n",
            " |              that will be invoked with the entire output dict of this ``Runnable``.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable``.\n",
            " |\n",
            " |  async astream_events(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, version: \"Literal['v1', 'v2']\" = 'v2', include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'AsyncIterator[StreamEvent]'\n",
            " |      Generate a stream of events.\n",
            " |\n",
            " |      Use to create an iterator over ``StreamEvents`` that provide real-time information\n",
            " |      about the progress of the ``Runnable``, including ``StreamEvents`` from intermediate\n",
            " |      results.\n",
            " |\n",
            " |      A ``StreamEvent`` is a dictionary with the following schema:\n",
            " |\n",
            " |      - ``event``: **str** - Event names are of the format:\n",
            " |        ``on_[runnable_type]_(start|stream|end)``.\n",
            " |      - ``name``: **str** - The name of the ``Runnable`` that generated the event.\n",
            " |      - ``run_id``: **str** - randomly generated ID associated with the given\n",
            " |        execution of the ``Runnable`` that emitted the event. A child ``Runnable`` that gets\n",
            " |        invoked as part of the execution of a parent ``Runnable`` is assigned its own\n",
            " |        unique ID.\n",
            " |      - ``parent_ids``: **list[str]** - The IDs of the parent runnables that generated\n",
            " |        the event. The root ``Runnable`` will have an empty list. The order of the parent\n",
            " |        IDs is from the root to the immediate parent. Only available for v2 version of\n",
            " |        the API. The v1 version of the API will return an empty list.\n",
            " |      - ``tags``: **Optional[list[str]]** - The tags of the ``Runnable`` that generated\n",
            " |        the event.\n",
            " |      - ``metadata``: **Optional[dict[str, Any]]** - The metadata of the ``Runnable`` that\n",
            " |        generated the event.\n",
            " |      - ``data``: **dict[str, Any]**\n",
            " |\n",
            " |      Below is a table that illustrates some events that might be emitted by various\n",
            " |      chains. Metadata fields have been omitted from the table for brevity.\n",
            " |      Chain definitions have been included after the table.\n",
            " |\n",
            " |      .. note::\n",
            " |          This reference table is for the v2 version of the schema.\n",
            " |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | event                    | name             | chunk                               | input                                             | output                                              |\n",
            " |      +==========================+==================+=====================================+===================================================+=====================================================+\n",
            " |      | ``on_chat_model_start``  | [model name]     |                                     | ``{\"messages\": [[SystemMessage, HumanMessage]]}`` |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_chat_model_stream`` | [model name]     | ``AIMessageChunk(content=\"hello\")`` |                                                   |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_chat_model_end``    | [model name]     |                                     | ``{\"messages\": [[SystemMessage, HumanMessage]]}`` | ``AIMessageChunk(content=\"hello world\")``           |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_llm_start``         | [model name]     |                                     | ``{'input': 'hello'}``                            |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_llm_stream``        | [model name]     | ``'Hello' ``                        |                                                   |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_llm_end``           | [model name]     |                                     | ``'Hello human!'``                                |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_chain_start``       | format_docs      |                                     |                                                   |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_chain_stream``      | format_docs      | ``'hello world!, goodbye world!'``  |                                                   |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_chain_end``         | format_docs      |                                     | ``[Document(...)]``                               | ``'hello world!, goodbye world!'``                  |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_tool_start``        | some_tool        |                                     | ``{\"x\": 1, \"y\": \"2\"}``                            |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_tool_end``          | some_tool        |                                     |                                                   | ``{\"x\": 1, \"y\": \"2\"}``                              |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_retriever_start``   | [retriever name] |                                     | ``{\"query\": \"hello\"}``                            |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_retriever_end``     | [retriever name] |                                     | ``{\"query\": \"hello\"}``                            | ``[Document(...), ..]``                             |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_prompt_start``      | [template_name]  |                                     | ``{\"question\": \"hello\"}``                         |                                                     |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |      | ``on_prompt_end``        | [template_name]  |                                     | ``{\"question\": \"hello\"}``                         | ``ChatPromptValue(messages: [SystemMessage, ...])`` |\n",
            " |      +--------------------------+------------------+-------------------------------------+---------------------------------------------------+-----------------------------------------------------+\n",
            " |\n",
            " |      In addition to the standard events, users can also dispatch custom events (see example below).\n",
            " |\n",
            " |      Custom events will be only be surfaced with in the v2 version of the API!\n",
            " |\n",
            " |      A custom event has following format:\n",
            " |\n",
            " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
            " |      | Attribute | Type | Description                                                                                               |\n",
            " |      +===========+======+===========================================================================================================+\n",
            " |      | name      | str  | A user defined name for the event.                                                                        |\n",
            " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
            " |      | data      | Any  | The data associated with the event. This can be anything, though we suggest making it JSON serializable.  |\n",
            " |      +-----------+------+-----------------------------------------------------------------------------------------------------------+\n",
            " |\n",
            " |      Here are declarations associated with the standard events shown above:\n",
            " |\n",
            " |      ``format_docs``:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          def format_docs(docs: list[Document]) -> str:\n",
            " |              '''Format the docs.'''\n",
            " |              return \", \".join([doc.page_content for doc in docs])\n",
            " |\n",
            " |\n",
            " |          format_docs = RunnableLambda(format_docs)\n",
            " |\n",
            " |      ``some_tool``:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          @tool\n",
            " |          def some_tool(x: int, y: str) -> dict:\n",
            " |              '''Some_tool.'''\n",
            " |              return {\"x\": x, \"y\": y}\n",
            " |\n",
            " |      ``prompt``:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          template = ChatPromptTemplate.from_messages(\n",
            " |              [\n",
            " |                  (\"system\", \"You are Cat Agent 007\"),\n",
            " |                  (\"human\", \"{question}\"),\n",
            " |              ]\n",
            " |          ).with_config({\"run_name\": \"my_template\", \"tags\": [\"my_template\"]})\n",
            " |\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |          async def reverse(s: str) -> str:\n",
            " |              return s[::-1]\n",
            " |\n",
            " |\n",
            " |          chain = RunnableLambda(func=reverse)\n",
            " |\n",
            " |          events = [\n",
            " |              event async for event in chain.astream_events(\"hello\", version=\"v2\")\n",
            " |          ]\n",
            " |\n",
            " |          # will produce the following events (run_id, and parent_ids\n",
            " |          # has been omitted for brevity):\n",
            " |          [\n",
            " |              {\n",
            " |                  \"data\": {\"input\": \"hello\"},\n",
            " |                  \"event\": \"on_chain_start\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |              {\n",
            " |                  \"data\": {\"chunk\": \"olleh\"},\n",
            " |                  \"event\": \"on_chain_stream\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |              {\n",
            " |                  \"data\": {\"output\": \"olleh\"},\n",
            " |                  \"event\": \"on_chain_end\",\n",
            " |                  \"metadata\": {},\n",
            " |                  \"name\": \"reverse\",\n",
            " |                  \"tags\": [],\n",
            " |              },\n",
            " |          ]\n",
            " |\n",
            " |\n",
            " |      Example: Dispatch Custom Event\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_core.callbacks.manager import (\n",
            " |              adispatch_custom_event,\n",
            " |          )\n",
            " |          from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
            " |          import asyncio\n",
            " |\n",
            " |\n",
            " |          async def slow_thing(some_input: str, config: RunnableConfig) -> str:\n",
            " |              \"\"\"Do something that takes a long time.\"\"\"\n",
            " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
            " |              await adispatch_custom_event(\n",
            " |                  \"progress_event\",\n",
            " |                  {\"message\": \"Finished step 1 of 3\"},\n",
            " |                  config=config # Must be included for python < 3.10\n",
            " |              )\n",
            " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
            " |              await adispatch_custom_event(\n",
            " |                  \"progress_event\",\n",
            " |                  {\"message\": \"Finished step 2 of 3\"},\n",
            " |                  config=config # Must be included for python < 3.10\n",
            " |              )\n",
            " |              await asyncio.sleep(1) # Placeholder for some slow operation\n",
            " |              return \"Done\"\n",
            " |\n",
            " |          slow_thing = RunnableLambda(slow_thing)\n",
            " |\n",
            " |          async for event in slow_thing.astream_events(\"some_input\", version=\"v2\"):\n",
            " |              print(event)\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the ``Runnable``.\n",
            " |          config: The config to use for the ``Runnable``.\n",
            " |          version: The version of the schema to use either ``'v2'`` or ``'v1'``.\n",
            " |                   Users should use ``'v2'``.\n",
            " |                   ``'v1'`` is for backwards compatibility and will be deprecated\n",
            " |                   in 0.4.0.\n",
            " |                   No default will be assigned until the API is stabilized.\n",
            " |                   custom events will only be surfaced in ``'v2'``.\n",
            " |          include_names: Only include events from ``Runnables`` with matching names.\n",
            " |          include_types: Only include events from ``Runnables`` with matching types.\n",
            " |          include_tags: Only include events from ``Runnables`` with matching tags.\n",
            " |          exclude_names: Exclude events from ``Runnables`` with matching names.\n",
            " |          exclude_types: Exclude events from ``Runnables`` with matching types.\n",
            " |          exclude_tags: Exclude events from ``Runnables`` with matching tags.\n",
            " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |              These will be passed to ``astream_log`` as this implementation\n",
            " |              of ``astream_events`` is built on top of ``astream_log``.\n",
            " |\n",
            " |      Yields:\n",
            " |          An async stream of ``StreamEvents``.\n",
            " |\n",
            " |      Raises:\n",
            " |          NotImplementedError: If the version is not ``'v1'`` or ``'v2'``.\n",
            " |\n",
            " |  async astream_log(self, input: 'Any', config: 'Optional[RunnableConfig]' = None, *, diff: 'bool' = True, with_streamed_output_list: 'bool' = True, include_names: 'Optional[Sequence[str]]' = None, include_types: 'Optional[Sequence[str]]' = None, include_tags: 'Optional[Sequence[str]]' = None, exclude_names: 'Optional[Sequence[str]]' = None, exclude_types: 'Optional[Sequence[str]]' = None, exclude_tags: 'Optional[Sequence[str]]' = None, **kwargs: 'Any') -> 'Union[AsyncIterator[RunLogPatch], AsyncIterator[RunLog]]'\n",
            " |      Stream all output from a ``Runnable``, as reported to the callback system.\n",
            " |\n",
            " |      This includes all inner runs of LLMs, Retrievers, Tools, etc.\n",
            " |\n",
            " |      Output is streamed as Log objects, which include a list of\n",
            " |      Jsonpatch ops that describe how the state of the run has changed in each\n",
            " |      step, and the final state of the run.\n",
            " |\n",
            " |      The Jsonpatch ops can be applied in order to construct state.\n",
            " |\n",
            " |      Args:\n",
            " |          input: The input to the ``Runnable``.\n",
            " |          config: The config to use for the ``Runnable``.\n",
            " |          diff: Whether to yield diffs between each step or the current state.\n",
            " |          with_streamed_output_list: Whether to yield the ``streamed_output`` list.\n",
            " |          include_names: Only include logs with these names.\n",
            " |          include_types: Only include logs with these types.\n",
            " |          include_tags: Only include logs with these tags.\n",
            " |          exclude_names: Exclude logs with these names.\n",
            " |          exclude_types: Exclude logs with these types.\n",
            " |          exclude_tags: Exclude logs with these tags.\n",
            " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Yields:\n",
            " |          A ``RunLogPatch`` or ``RunLog`` object.\n",
            " |\n",
            " |  async atransform(self, input: 'AsyncIterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'AsyncIterator[Output]'\n",
            " |      Transform inputs to outputs.\n",
            " |\n",
            " |      Default implementation of atransform, which buffers input and calls ``astream``.\n",
            " |\n",
            " |      Subclasses should override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |\n",
            " |      Args:\n",
            " |          input: An async iterator of inputs to the ``Runnable``.\n",
            " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
            " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Yields:\n",
            " |          The output of the ``Runnable``.\n",
            " |\n",
            " |  batch(self, inputs: 'list[Input]', config: 'Optional[Union[RunnableConfig, list[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'list[Output]'\n",
            " |      Default implementation runs invoke in parallel using a thread pool executor.\n",
            " |\n",
            " |      The default implementation of batch works well for IO bound runnables.\n",
            " |\n",
            " |      Subclasses should override this method if they can batch more efficiently;\n",
            " |      e.g., if the underlying ``Runnable`` uses an API which supports a batch mode.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs: A list of inputs to the ``Runnable``.\n",
            " |          config: A config to use when invoking the ``Runnable``. The config supports\n",
            " |              standard keys like ``'tags'``, ``'metadata'`` for\n",
            " |              tracing purposes, ``'max_concurrency'`` for controlling how much work\n",
            " |              to do in parallel, and other keys. Please refer to the\n",
            " |              ``RunnableConfig`` for more details. Defaults to None.\n",
            " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
            " |              Defaults to False.\n",
            " |          **kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Returns:\n",
            " |          A list of outputs from the ``Runnable``.\n",
            " |\n",
            " |  batch_as_completed(self, inputs: 'Sequence[Input]', config: 'Optional[Union[RunnableConfig, Sequence[RunnableConfig]]]' = None, *, return_exceptions: 'bool' = False, **kwargs: 'Optional[Any]') -> 'Iterator[tuple[int, Union[Output, Exception]]]'\n",
            " |      Run ``invoke`` in parallel on a list of inputs.\n",
            " |\n",
            " |      Yields results as they complete.\n",
            " |\n",
            " |      Args:\n",
            " |          inputs: A list of inputs to the ``Runnable``.\n",
            " |          config: A config to use when invoking the ``Runnable``.\n",
            " |              The config supports standard keys like ``'tags'``, ``'metadata'`` for\n",
            " |              tracing purposes, ``'max_concurrency'`` for controlling how much work to\n",
            " |              do in parallel, and other keys. Please refer to the ``RunnableConfig``\n",
            " |              for more details. Defaults to None.\n",
            " |          return_exceptions: Whether to return exceptions instead of raising them.\n",
            " |              Defaults to False.\n",
            " |          **kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Yields:\n",
            " |          Tuples of the index of the input and the output from the ``Runnable``.\n",
            " |\n",
            " |  bind(self, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind arguments to a ``Runnable``, returning a new ``Runnable``.\n",
            " |\n",
            " |      Useful when a ``Runnable`` in a chain requires an argument that is not\n",
            " |      in the output of the previous ``Runnable`` or included in the user input.\n",
            " |\n",
            " |      Args:\n",
            " |          kwargs: The arguments to bind to the ``Runnable``.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable`` with the arguments bound.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_ollama import ChatOllama\n",
            " |          from langchain_core.output_parsers import StrOutputParser\n",
            " |\n",
            " |          llm = ChatOllama(model=\"llama2\")\n",
            " |\n",
            " |          # Without bind.\n",
            " |          chain = llm | StrOutputParser()\n",
            " |\n",
            " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
            " |          # Output is 'One two three four five.'\n",
            " |\n",
            " |          # With bind.\n",
            " |          chain = llm.bind(stop=[\"three\"]) | StrOutputParser()\n",
            " |\n",
            " |          chain.invoke(\"Repeat quoted words exactly: 'One two three four five.'\")\n",
            " |          # Output is 'One two'\n",
            " |\n",
            " |  config_schema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'type[BaseModel]'\n",
            " |      The type of config this ``Runnable`` accepts specified as a pydantic model.\n",
            " |\n",
            " |      To mark a field as configurable, see the ``configurable_fields``\n",
            " |      and ``configurable_alternatives`` methods.\n",
            " |\n",
            " |      Args:\n",
            " |          include: A list of fields to include in the config schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A pydantic model that can be used to validate config.\n",
            " |\n",
            " |  get_config_jsonschema(self, *, include: 'Optional[Sequence[str]]' = None) -> 'dict[str, Any]'\n",
            " |      Get a JSON schema that represents the config of the ``Runnable``.\n",
            " |\n",
            " |      Args:\n",
            " |          include: A list of fields to include in the config schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON schema that represents the config of the ``Runnable``.\n",
            " |\n",
            " |      .. versionadded:: 0.3.0\n",
            " |\n",
            " |  get_graph(self, config: 'Optional[RunnableConfig]' = None) -> 'Graph'\n",
            " |      Return a graph representation of this ``Runnable``.\n",
            " |\n",
            " |  get_input_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
            " |      Get a JSON schema that represents the input to the ``Runnable``.\n",
            " |\n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON schema that represents the input to the ``Runnable``.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |              from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |              def add_one(x: int) -> int:\n",
            " |                  return x + 1\n",
            " |\n",
            " |\n",
            " |              runnable = RunnableLambda(add_one)\n",
            " |\n",
            " |              print(runnable.get_input_jsonschema())\n",
            " |\n",
            " |      .. versionadded:: 0.3.0\n",
            " |\n",
            " |  get_input_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
            " |      Get a pydantic model that can be used to validate input to the Runnable.\n",
            " |\n",
            " |      ``Runnable``s that leverage the ``configurable_fields`` and\n",
            " |      ``configurable_alternatives`` methods will have a dynamic input schema that\n",
            " |      depends on which configuration the ``Runnable`` is invoked with.\n",
            " |\n",
            " |      This method allows to get an input schema for a specific configuration.\n",
            " |\n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A pydantic model that can be used to validate input.\n",
            " |\n",
            " |  get_name(self, suffix: 'Optional[str]' = None, *, name: 'Optional[str]' = None) -> 'str'\n",
            " |      Get the name of the ``Runnable``.\n",
            " |\n",
            " |      Args:\n",
            " |          suffix: An optional suffix to append to the name.\n",
            " |          name: An optional name to use instead of the ``Runnable``'s name.\n",
            " |\n",
            " |      Returns:\n",
            " |          The name of the ``Runnable``.\n",
            " |\n",
            " |  get_output_jsonschema(self, config: 'Optional[RunnableConfig]' = None) -> 'dict[str, Any]'\n",
            " |      Get a JSON schema that represents the output of the ``Runnable``.\n",
            " |\n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A JSON schema that represents the output of the ``Runnable``.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |              from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |              def add_one(x: int) -> int:\n",
            " |                  return x + 1\n",
            " |\n",
            " |\n",
            " |              runnable = RunnableLambda(add_one)\n",
            " |\n",
            " |              print(runnable.get_output_jsonschema())\n",
            " |\n",
            " |      .. versionadded:: 0.3.0\n",
            " |\n",
            " |  get_output_schema(self, config: 'Optional[RunnableConfig]' = None) -> 'type[BaseModel]'\n",
            " |      Get a pydantic model that can be used to validate output to the ``Runnable``.\n",
            " |\n",
            " |      ``Runnable``s that leverage the ``configurable_fields`` and\n",
            " |      ``configurable_alternatives`` methods will have a dynamic output schema that\n",
            " |      depends on which configuration the ``Runnable`` is invoked with.\n",
            " |\n",
            " |      This method allows to get an output schema for a specific configuration.\n",
            " |\n",
            " |      Args:\n",
            " |          config: A config to use when generating the schema.\n",
            " |\n",
            " |      Returns:\n",
            " |          A pydantic model that can be used to validate output.\n",
            " |\n",
            " |  get_prompts(self, config: 'Optional[RunnableConfig]' = None) -> 'list[BasePromptTemplate]'\n",
            " |      Return a list of prompts used by this ``Runnable``.\n",
            " |\n",
            " |  map(self) -> 'Runnable[list[Input], list[Output]]'\n",
            " |      Return a new ``Runnable`` that maps a list of inputs to a list of outputs.\n",
            " |\n",
            " |      Calls ``invoke`` with each input.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable`` that maps a list of inputs to a list of outputs.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |                  from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |                  def _lambda(x: int) -> int:\n",
            " |                      return x + 1\n",
            " |\n",
            " |\n",
            " |                  runnable = RunnableLambda(_lambda)\n",
            " |                  print(runnable.map().invoke([1, 2, 3]))  # [2, 3, 4]\n",
            " |\n",
            " |  pick(self, keys: 'Union[str, list[str]]') -> 'RunnableSerializable[Any, Any]'\n",
            " |      Pick keys from the output dict of this ``Runnable``.\n",
            " |\n",
            " |      Pick single key:\n",
            " |\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |              import json\n",
            " |\n",
            " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
            " |\n",
            " |              as_str = RunnableLambda(str)\n",
            " |              as_json = RunnableLambda(json.loads)\n",
            " |              chain = RunnableMap(str=as_str, json=as_json)\n",
            " |\n",
            " |              chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3]}\n",
            " |\n",
            " |              json_only_chain = chain.pick(\"json\")\n",
            " |              json_only_chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> [1, 2, 3]\n",
            " |\n",
            " |      Pick list of keys:\n",
            " |\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |              from typing import Any\n",
            " |\n",
            " |              import json\n",
            " |\n",
            " |              from langchain_core.runnables import RunnableLambda, RunnableMap\n",
            " |\n",
            " |              as_str = RunnableLambda(str)\n",
            " |              as_json = RunnableLambda(json.loads)\n",
            " |\n",
            " |\n",
            " |              def as_bytes(x: Any) -> bytes:\n",
            " |                  return bytes(x, \"utf-8\")\n",
            " |\n",
            " |\n",
            " |              chain = RunnableMap(\n",
            " |                  str=as_str, json=as_json, bytes=RunnableLambda(as_bytes)\n",
            " |              )\n",
            " |\n",
            " |              chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> {\"str\": \"[1, 2, 3]\", \"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
            " |\n",
            " |              json_and_bytes_chain = chain.pick([\"json\", \"bytes\"])\n",
            " |              json_and_bytes_chain.invoke(\"[1, 2, 3]\")\n",
            " |              # -> {\"json\": [1, 2, 3], \"bytes\": b\"[1, 2, 3]\"}\n",
            " |\n",
            " |      Args:\n",
            " |          keys: A key or list of keys to pick from the output dict.\n",
            " |\n",
            " |      Returns:\n",
            " |          a new ``Runnable``.\n",
            " |\n",
            " |  pipe(self, *others: 'Union[Runnable[Any, Other], Callable[[Any], Other]]', name: 'Optional[str]' = None) -> 'RunnableSerializable[Input, Other]'\n",
            " |      Pipe runnables.\n",
            " |\n",
            " |      Compose this ``Runnable`` with ``Runnable``-like objects to make a\n",
            " |      ``RunnableSequence``.\n",
            " |\n",
            " |      Equivalent to ``RunnableSequence(self, *others)`` or ``self | others[0] | ...``\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |              from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |\n",
            " |              def add_one(x: int) -> int:\n",
            " |                  return x + 1\n",
            " |\n",
            " |\n",
            " |              def mul_two(x: int) -> int:\n",
            " |                  return x * 2\n",
            " |\n",
            " |\n",
            " |              runnable_1 = RunnableLambda(add_one)\n",
            " |              runnable_2 = RunnableLambda(mul_two)\n",
            " |              sequence = runnable_1.pipe(runnable_2)\n",
            " |              # Or equivalently:\n",
            " |              # sequence = runnable_1 | runnable_2\n",
            " |              # sequence = RunnableSequence(first=runnable_1, last=runnable_2)\n",
            " |              sequence.invoke(1)\n",
            " |              await sequence.ainvoke(1)\n",
            " |              # -> 4\n",
            " |\n",
            " |              sequence.batch([1, 2, 3])\n",
            " |              await sequence.abatch([1, 2, 3])\n",
            " |              # -> [4, 6, 8]\n",
            " |\n",
            " |      Args:\n",
            " |          *others: Other ``Runnable`` or ``Runnable``-like objects to compose\n",
            " |          name: An optional name for the resulting ``RunnableSequence``.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable``.\n",
            " |\n",
            " |  transform(self, input: 'Iterator[Input]', config: 'Optional[RunnableConfig]' = None, **kwargs: 'Optional[Any]') -> 'Iterator[Output]'\n",
            " |      Transform inputs to outputs.\n",
            " |\n",
            " |      Default implementation of transform, which buffers input and calls ``astream``.\n",
            " |\n",
            " |      Subclasses should override this method if they can start producing output while\n",
            " |      input is still being generated.\n",
            " |\n",
            " |      Args:\n",
            " |          input: An iterator of inputs to the ``Runnable``.\n",
            " |          config: The config to use for the ``Runnable``. Defaults to None.\n",
            " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Yields:\n",
            " |          The output of the ``Runnable``.\n",
            " |\n",
            " |  with_alisteners(self, *, on_start: 'Optional[AsyncListener]' = None, on_end: 'Optional[AsyncListener]' = None, on_error: 'Optional[AsyncListener]' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind async lifecycle listeners to a ``Runnable``.\n",
            " |\n",
            " |      Returns a new ``Runnable``.\n",
            " |\n",
            " |      The Run object contains information about the run, including its ``id``,\n",
            " |      ``type``, ``input``, ``output``, ``error``, ``start_time``, ``end_time``, and\n",
            " |      any tags or metadata added to the run.\n",
            " |\n",
            " |      Args:\n",
            " |          on_start: Called asynchronously before the ``Runnable`` starts running,\n",
            " |              with the ``Run`` object. Defaults to None.\n",
            " |          on_end: Called asynchronously after the ``Runnable`` finishes running,\n",
            " |              with the ``Run`` object. Defaults to None.\n",
            " |          on_error: Called asynchronously if the ``Runnable`` throws an error,\n",
            " |              with the ``Run`` object. Defaults to None.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable`` with the listeners bound.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_core.runnables import RunnableLambda, Runnable\n",
            " |          from datetime import datetime, timezone\n",
            " |          import time\n",
            " |          import asyncio\n",
            " |\n",
            " |          def format_t(timestamp: float) -> str:\n",
            " |              return datetime.fromtimestamp(timestamp, tz=timezone.utc).isoformat()\n",
            " |\n",
            " |          async def test_runnable(time_to_sleep : int):\n",
            " |              print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
            " |              await asyncio.sleep(time_to_sleep)\n",
            " |              print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
            " |\n",
            " |          async def fn_start(run_obj : Runnable):\n",
            " |              print(f\"on start callback starts at {format_t(time.time())}\")\n",
            " |              await asyncio.sleep(3)\n",
            " |              print(f\"on start callback ends at {format_t(time.time())}\")\n",
            " |\n",
            " |          async def fn_end(run_obj : Runnable):\n",
            " |              print(f\"on end callback starts at {format_t(time.time())}\")\n",
            " |              await asyncio.sleep(2)\n",
            " |              print(f\"on end callback ends at {format_t(time.time())}\")\n",
            " |\n",
            " |          runnable = RunnableLambda(test_runnable).with_alisteners(\n",
            " |              on_start=fn_start,\n",
            " |              on_end=fn_end\n",
            " |          )\n",
            " |          async def concurrent_runs():\n",
            " |              await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
            " |\n",
            " |          asyncio.run(concurrent_runs())\n",
            " |          Result:\n",
            " |          on start callback starts at 2025-03-01T07:05:22.875378+00:00\n",
            " |          on start callback starts at 2025-03-01T07:05:22.875495+00:00\n",
            " |          on start callback ends at 2025-03-01T07:05:25.878862+00:00\n",
            " |          on start callback ends at 2025-03-01T07:05:25.878947+00:00\n",
            " |          Runnable[2s]: starts at 2025-03-01T07:05:25.879392+00:00\n",
            " |          Runnable[3s]: starts at 2025-03-01T07:05:25.879804+00:00\n",
            " |          Runnable[2s]: ends at 2025-03-01T07:05:27.881998+00:00\n",
            " |          on end callback starts at 2025-03-01T07:05:27.882360+00:00\n",
            " |          Runnable[3s]: ends at 2025-03-01T07:05:28.881737+00:00\n",
            " |          on end callback starts at 2025-03-01T07:05:28.882428+00:00\n",
            " |          on end callback ends at 2025-03-01T07:05:29.883893+00:00\n",
            " |          on end callback ends at 2025-03-01T07:05:30.884831+00:00\n",
            " |\n",
            " |  with_config(self, config: 'Optional[RunnableConfig]' = None, **kwargs: 'Any') -> 'Runnable[Input, Output]'\n",
            " |      Bind config to a ``Runnable``, returning a new ``Runnable``.\n",
            " |\n",
            " |      Args:\n",
            " |          config: The config to bind to the ``Runnable``.\n",
            " |          kwargs: Additional keyword arguments to pass to the ``Runnable``.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable`` with the config bound.\n",
            " |\n",
            " |  with_fallbacks(self, fallbacks: 'Sequence[Runnable[Input, Output]]', *, exceptions_to_handle: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), exception_key: 'Optional[str]' = None) -> 'RunnableWithFallbacksT[Input, Output]'\n",
            " |      Add fallbacks to a ``Runnable``, returning a new ``Runnable``.\n",
            " |\n",
            " |      The new ``Runnable`` will try the original ``Runnable``, and then each fallback\n",
            " |      in order, upon failures.\n",
            " |\n",
            " |      Args:\n",
            " |          fallbacks: A sequence of runnables to try if the original ``Runnable``\n",
            " |              fails.\n",
            " |          exceptions_to_handle: A tuple of exception types to handle.\n",
            " |              Defaults to ``(Exception,)``.\n",
            " |          exception_key: If string is specified then handled exceptions will be passed\n",
            " |              to fallbacks as part of the input under the specified key.\n",
            " |              If None, exceptions will not be passed to fallbacks.\n",
            " |              If used, the base ``Runnable`` and its fallbacks must accept a\n",
            " |              dictionary as input. Defaults to None.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable`` that will try the original ``Runnable``, and then each\n",
            " |          fallback in order, upon failures.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |          .. code-block:: python\n",
            " |\n",
            " |              from typing import Iterator\n",
            " |\n",
            " |              from langchain_core.runnables import RunnableGenerator\n",
            " |\n",
            " |\n",
            " |              def _generate_immediate_error(input: Iterator) -> Iterator[str]:\n",
            " |                  raise ValueError()\n",
            " |                  yield \"\"\n",
            " |\n",
            " |\n",
            " |              def _generate(input: Iterator) -> Iterator[str]:\n",
            " |                  yield from \"foo bar\"\n",
            " |\n",
            " |\n",
            " |              runnable = RunnableGenerator(_generate_immediate_error).with_fallbacks(\n",
            " |                  [RunnableGenerator(_generate)]\n",
            " |              )\n",
            " |              print(\"\".join(runnable.stream({})))  # foo bar\n",
            " |\n",
            " |      Args:\n",
            " |          fallbacks: A sequence of runnables to try if the original ``Runnable``\n",
            " |              fails.\n",
            " |          exceptions_to_handle: A tuple of exception types to handle.\n",
            " |          exception_key: If string is specified then handled exceptions will be passed\n",
            " |              to fallbacks as part of the input under the specified key.\n",
            " |              If None, exceptions will not be passed to fallbacks.\n",
            " |              If used, the base ``Runnable`` and its fallbacks must accept a\n",
            " |              dictionary as input.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable`` that will try the original ``Runnable``, and then each\n",
            " |          fallback in order, upon failures.\n",
            " |\n",
            " |  with_listeners(self, *, on_start: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_end: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None, on_error: 'Optional[Union[Callable[[Run], None], Callable[[Run, RunnableConfig], None]]]' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind lifecycle listeners to a ``Runnable``, returning a new ``Runnable``.\n",
            " |\n",
            " |      The Run object contains information about the run, including its ``id``,\n",
            " |      ``type``, ``input``, ``output``, ``error``, ``start_time``, ``end_time``, and\n",
            " |      any tags or metadata added to the run.\n",
            " |\n",
            " |      Args:\n",
            " |          on_start: Called before the ``Runnable`` starts running, with the ``Run``\n",
            " |              object. Defaults to None.\n",
            " |          on_end: Called after the ``Runnable`` finishes running, with the ``Run``\n",
            " |              object. Defaults to None.\n",
            " |          on_error: Called if the ``Runnable`` throws an error, with the ``Run``\n",
            " |              object. Defaults to None.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new ``Runnable`` with the listeners bound.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |          from langchain_core.tracers.schemas import Run\n",
            " |\n",
            " |          import time\n",
            " |\n",
            " |\n",
            " |          def test_runnable(time_to_sleep: int):\n",
            " |              time.sleep(time_to_sleep)\n",
            " |\n",
            " |\n",
            " |          def fn_start(run_obj: Run):\n",
            " |              print(\"start_time:\", run_obj.start_time)\n",
            " |\n",
            " |\n",
            " |          def fn_end(run_obj: Run):\n",
            " |              print(\"end_time:\", run_obj.end_time)\n",
            " |\n",
            " |\n",
            " |          chain = RunnableLambda(test_runnable).with_listeners(\n",
            " |              on_start=fn_start, on_end=fn_end\n",
            " |          )\n",
            " |          chain.invoke(2)\n",
            " |\n",
            " |  with_retry(self, *, retry_if_exception_type: 'tuple[type[BaseException], ...]' = (<class 'Exception'>,), wait_exponential_jitter: 'bool' = True, exponential_jitter_params: 'Optional[ExponentialJitterParams]' = None, stop_after_attempt: 'int' = 3) -> 'Runnable[Input, Output]'\n",
            " |      Create a new Runnable that retries the original Runnable on exceptions.\n",
            " |\n",
            " |      Args:\n",
            " |          retry_if_exception_type: A tuple of exception types to retry on.\n",
            " |              Defaults to (Exception,).\n",
            " |          wait_exponential_jitter: Whether to add jitter to the wait\n",
            " |              time between retries. Defaults to True.\n",
            " |          stop_after_attempt: The maximum number of attempts to make before\n",
            " |              giving up. Defaults to 3.\n",
            " |          exponential_jitter_params: Parameters for\n",
            " |              ``tenacity.wait_exponential_jitter``. Namely: ``initial``, ``max``,\n",
            " |              ``exp_base``, and ``jitter`` (all float values).\n",
            " |\n",
            " |      Returns:\n",
            " |          A new Runnable that retries the original Runnable on exceptions.\n",
            " |\n",
            " |      Example:\n",
            " |\n",
            " |      .. code-block:: python\n",
            " |\n",
            " |          from langchain_core.runnables import RunnableLambda\n",
            " |\n",
            " |          count = 0\n",
            " |\n",
            " |\n",
            " |          def _lambda(x: int) -> None:\n",
            " |              global count\n",
            " |              count = count + 1\n",
            " |              if x == 1:\n",
            " |                  raise ValueError(\"x is 1\")\n",
            " |              else:\n",
            " |                  pass\n",
            " |\n",
            " |\n",
            " |          runnable = RunnableLambda(_lambda)\n",
            " |          try:\n",
            " |              runnable.with_retry(\n",
            " |                  stop_after_attempt=2,\n",
            " |                  retry_if_exception_type=(ValueError,),\n",
            " |              ).invoke(1)\n",
            " |          except ValueError:\n",
            " |              pass\n",
            " |\n",
            " |          assert count == 2\n",
            " |\n",
            " |  with_types(self, *, input_type: 'Optional[type[Input]]' = None, output_type: 'Optional[type[Output]]' = None) -> 'Runnable[Input, Output]'\n",
            " |      Bind input and output types to a ``Runnable``, returning a new ``Runnable``.\n",
            " |\n",
            " |      Args:\n",
            " |          input_type: The input type to bind to the ``Runnable``. Defaults to None.\n",
            " |          output_type: The output type to bind to the ``Runnable``. Defaults to None.\n",
            " |\n",
            " |      Returns:\n",
            " |          A new Runnable with the types bound.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from langchain_core.runnables.base.Runnable:\n",
            " |\n",
            " |  config_specs\n",
            " |      List configurable fields for this ``Runnable``.\n",
            " |\n",
            " |  input_schema\n",
            " |      The type of input this ``Runnable`` accepts specified as a pydantic model.\n",
            " |\n",
            " |  output_schema\n",
            " |      Output schema.\n",
            " |\n",
            " |      The type of output this ``Runnable`` produces specified as a pydantic model.\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |\n",
            " |  __init_subclass__(...)\n",
            " |      Function to initialize subclasses.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "WPiJiremScD5"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### First Test"
      ],
      "metadata": {
        "id": "-F08gJvPoQhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# 1. Initialize the Chat Model (from the previous step)\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Prepare the messages\n",
        "# The SystemMessage sets the behavior and context for the AI.\n",
        "# The HumanMessage is the user's actual query.\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"),\n",
        "    HumanMessage(content=\"What is Ayushman Bharat?\"),\n",
        "]\n",
        "\n",
        "# 3. Invoke the model with the messages\n",
        "result = chat_model.invoke(messages) # notice the similarity with model.predict from sklearn\n",
        "\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "4N2YE7g0UVqd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7ad399b-8552-4ab8-a6df-cdfb7f656150"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ayushman Bharat is a flagship national health protection scheme launched by the Government of India in 2018. Its primary goal is to achieve Universal Health Coverage (UHC) and ensure that no one is left behind due to financial constraints when seeking healthcare.\n",
            "\n",
            "The scheme has two main components:\n",
            "\n",
            "1.  **Pradhan Mantri Jan Arogya Yojana (PMJAY):** This is the health insurance component, often referred to as the world's largest government-funded health insurance scheme.\n",
            "    *   **Objective:** To provide financial protection to over 50 crore (500 million) poor and vulnerable families for secondary and tertiary care hospitalization.\n",
            "    *   **Coverage:** It offers a health cover of up to 5 lakh (approximately $6,000 USD) per family per year for cashless and paperless treatment at empanelled public and private hospitals across India.\n",
            "    *   **Benefits:** Covers pre-hospitalization expenses, hospitalization expenses, and post-hospitalization expenses for a wide range of medical and surgical procedures.\n",
            "    *   **Eligibility:** Based on the Socio-Economic Caste Census (SECC) 2011 data, targeting specific deprivation and occupational criteria in rural and urban areas, respectively.\n",
            "\n",
            "2.  **Ayushman Bharat Health and Wellness Centres (AB-HWCs):** This component aims to transform existing Sub Centres and Primary Health Centres into Health and Wellness Centres.\n",
            "    *   **Objective:** To bring healthcare closer to the homes of people, focusing on comprehensive primary healthcare.\n",
            "    *   **Services:** They provide a wide range of services, including:\n",
            "        *   Maternal and child health services.\n",
            "        *   Non-communicable diseases (NCDs) screening and management (e.g., hypertension, diabetes, common cancers).\n",
            "        *   Care for the elderly.\n",
            "        *   Mental health services.\n",
            "        *   Oral health.\n",
            "        *   Eye care.\n",
            "        *   Emergency medical services.\n",
            "        *   Yoga and wellness activities.\n",
            "    *   **Focus:** Emphasizes preventive, promotive, curative, palliative, and rehabilitative care, moving beyond selective care to comprehensive care.\n",
            "\n",
            "In essence, Ayushman Bharat is a two-pronged approach to healthcare reform in India, addressing both the financial burden of hospitalization through PMJAY and strengthening primary healthcare delivery through AB-HWCs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model.invoke(\"What is blood pressure?\")"
      ],
      "metadata": {
        "id": "QzzuvFGqUn0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c52ddcb-02cb-49e4-cefd-dcd7764efcdb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Blood pressure is the **force of your blood pushing against the walls of your arteries** as your heart pumps it around your body.\\n\\nThink of it like water flowing through a hose:\\n*   The **heart is the pump** that pushes the water (blood).\\n*   The **arteries are the hose** through which the water flows.\\n*   **Blood pressure is the force** the water exerts on the inside walls of the hose.\\n\\nIt\\'s a vital sign because it indicates how hard your heart is working and how much resistance your blood vessels are providing.\\n\\n### The Two Numbers: Systolic and Diastolic\\n\\nWhen you get a blood pressure reading, you\\'ll see two numbers, one on top and one on the bottom (e.g., 120/80 mmHg):\\n\\n1.  **Systolic Pressure (the top number):**\\n    *   This is the **higher** number.\\n    *   It measures the pressure in your arteries when your **heart beats** (contracts) and pushes blood out.\\n    *   It represents the maximum pressure your blood exerts.\\n\\n2.  **Diastolic Pressure (the bottom number):**\\n    *   This is the **lower** number.\\n    *   It measures the pressure in your arteries when your **heart rests** between beats and refills with blood.\\n    *   It represents the minimum pressure your blood exerts.\\n\\nThe unit \"mmHg\" stands for millimeters of mercury, which is the standard unit of measurement for blood pressure.\\n\\n### Why is Blood Pressure Important?\\n\\n*   **Essential for Circulation:** Adequate blood pressure is necessary to ensure blood reaches all parts of your body, delivering oxygen and nutrients.\\n*   **Indicator of Heart Health:**\\n    *   **High blood pressure (hypertension):** Means your heart is working too hard, and the force on your artery walls is too great. Over time, this can damage arteries, leading to serious health problems like heart attack, stroke, kidney disease, and vision loss. It\\'s often called the \"silent killer\" because it usually has no symptoms.\\n    *   **Low blood pressure (hypotension):** Means there isn\\'t enough force to deliver blood effectively, which can cause dizziness, fainting, and in severe cases, organ damage.\\n\\n### How is it Measured?\\n\\nBlood pressure is typically measured using a **sphygmomanometer** (blood pressure cuff). The cuff is wrapped around your upper arm and inflated to temporarily stop blood flow. As the cuff slowly deflates, a healthcare professional (or a digital monitor) listens for or detects the sounds of blood flowing through the artery to determine the systolic and diastolic pressures.\\n\\nRegular blood pressure checks are crucial for monitoring your cardiovascular health and detecting potential issues early.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash', 'safety_ratings': []}, id='run--1f2df6c0-18e2-4f65-9938-7764a7eb373a-0', usage_metadata={'input_tokens': 6, 'output_tokens': 1457, 'total_tokens': 1463, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 879}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second Test"
      ],
      "metadata": {
        "id": "FmD3qL7_oTGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "# 1. Initialize the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Prepare messages with an out-of-scope question\n",
        "messages = [\n",
        "    SystemMessage(content=\"You're an assistant knowledgeable about healthcare. Only answer healthcare-related questions.\"),\n",
        "    HumanMessage(content=\"How do I change a tire?\"),\n",
        "]\n",
        "\n",
        "# 3. Invoke the model\n",
        "result = chat_model.invoke(messages)\n",
        "\n",
        "print(result.content)"
      ],
      "metadata": {
        "id": "y_eeP53XUFLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc0d8d21-a34e-44fd-86a5-a9c8bf89501b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I can only answer healthcare-related questions. Changing a tire is not a healthcare topic.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Initialize the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Create the Prompt Template\n",
        "instruction_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions. Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "review_template = ChatPromptTemplate.from_template(instruction_str)\n",
        "\n",
        "# 3. Define the context and question\n",
        "context = \"The discharge process was seamless!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "# 4. Create the chain by piping the components together\n",
        "#    We also add an output parser to get a clean string result.\n",
        "chain = review_template | chat_model | StrOutputParser()\n",
        "\n",
        "# 5. Invoke the chain with the input variables\n",
        "result = chain.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "pNAWRLXoTePQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9fc7485-37e4-4b4d-bb28-e107f948113a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, at least one patient had a positive experience. They specifically mentioned that \"The discharge process was seamless!\", indicating a smooth, easy, and problem-free experience with their discharge.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using PromptTemplates & MessageTemplates"
      ],
      "metadata": {
        "id": "--bZZ51YYb0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import (\n",
        "    PromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# 1. Initialize the Chat Model\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "\n",
        "# 2. Create the detailed prompt templates\n",
        "instruction_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions.\n",
        "Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "Context: {context}\n",
        "\"\"\"\n",
        "\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"], template=instruction_str\n",
        "    )\n",
        ")\n",
        "\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"question\"], template=\"{question}\"\n",
        "    )\n",
        ")\n",
        "\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "# This is our final, reusable prompt template\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    messages=messages,\n",
        ")\n",
        "\n",
        "# 3. Define the context and question\n",
        "context = \"I had a great stay!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "# 4. Create the chain\n",
        "chain = review_prompt_template | chat_model | StrOutputParser()\n",
        "\n",
        "# 5. Invoke the chain\n",
        "result = chain.invoke({\n",
        "    \"context\": context,\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(result)"
      ],
      "metadata": {
        "id": "GhrsnDNPTeMa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f039d169-5546-4122-c616-73fbef15d19c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yes, one patient stated, \"I had a great stay!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = \"I had a negative stay!\"\n",
        "question = \"Did anyone have a positive experience?\"\n",
        "\n",
        "chain.invoke({\"context\": context, \"question\": question})"
      ],
      "metadata": {
        "id": "hiKXXNE9Td0i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3746aa45-4f48-4cbe-fe56-a84762c3c7ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided context, the only information available is from one patient who stated, \"I had a negative stay!\" There is no information about anyone having a positive experience.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding RAG"
      ],
      "metadata": {
        "id": "3KH3JCq5ra_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain-chroma # FAISS"
      ],
      "metadata": {
        "collapsed": true,
        "id": "OD9upnVJkxDV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bed0ca3-b1a4-47b2-dc3d-c20dbc892c92"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-chroma in /usr/local/lib/python3.12/dist-packages (0.2.6)\n",
            "Requirement already satisfied: langchain-core>=0.3.76 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (0.3.77)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (2.0.2)\n",
            "Requirement already satisfied: chromadb>=1.0.20 in /usr/local/lib/python3.12/dist-packages (from langchain-chroma) (1.1.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (2.11.9)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.4.2)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (0.35.0)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (4.15.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.23.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (0.22.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (1.75.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (5.0.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (0.17.4)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (34.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (5.2.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (3.11.3)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.12/dist-packages (from chromadb>=1.0.20->langchain-chroma) (4.25.1)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.76->langchain-chroma) (0.4.28)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.76->langchain-chroma) (1.33)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core>=0.3.76->langchain-chroma) (25.0)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.12/dist-packages (from build>=1.0.3->chromadb>=1.0.20->langchain-chroma) (1.2.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core>=0.3.76->langchain-chroma) (3.0.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain-chroma) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain-chroma) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain-chroma) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.19.0->chromadb>=1.0.20->langchain-chroma) (0.27.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (2.32.5)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (2.0.0)\n",
            "Requirement already satisfied: urllib3<2.4.0,>=1.24.2 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.12/dist-packages (from kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (0.10)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.76->langchain-chroma) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core>=0.3.76->langchain-chroma) (0.25.0)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (1.13.3)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-api>=1.2.0->chromadb>=1.0.20->langchain-chroma) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain-chroma) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.37.0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb>=1.0.20->langchain-chroma) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb>=1.0.20->langchain-chroma) (0.58b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain-chroma) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb>=1.0.20->langchain-chroma) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain-chroma) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain-chroma) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=1.9->chromadb>=1.0.20->langchain-chroma) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain-chroma) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->chromadb>=1.0.20->langchain-chroma) (2.19.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers>=0.13.2->chromadb>=1.0.20->langchain-chroma) (0.35.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain-chroma) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer>=0.9.0->chromadb>=1.0.20->langchain-chroma) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.12/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=1.0.20->langchain-chroma) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain-chroma) (3.19.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain-chroma) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb>=1.0.20->langchain-chroma) (1.1.10)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb>=1.0.20->langchain-chroma) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=1.0.20->langchain-chroma) (0.1.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (3.4.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27.0->chromadb>=1.0.20->langchain-chroma) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (10.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from requests-oauthlib->kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (3.3.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=1.0.20->langchain-chroma) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=1.0.20->langchain-chroma) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests # Import requests to download the file\n",
        "import os # Import os to handle file paths\n",
        "from google.colab import userdata\n",
        "\n",
        "# Import the CSVLoader class to load documents from a CSV file.\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "\n",
        "# Import the Chroma class, which is used to create and interact with a Chroma vector database.\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "# Import the GoogleGenerativeAIEmbeddings class to create numerical vector representations (embeddings) of text using Google's models.\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "\n",
        "\n",
        "REVIEWS_CSV_PATH = \"https://raw.githubusercontent.com/GeeksforgeeksDS/21-Days-21-Projects-Dataset/main/Datasets/reviews.csv\" # Use the raw content URL\n",
        "\n",
        "# Define a constant variable for the directory where the Chroma vector database will be stored.\n",
        "REVIEWS_CHROMA_PATH = \"chroma_data\"\n",
        "\n",
        "# Define the local path to save the downloaded CSV\n",
        "LOCAL_REVIEWS_CSV_PATH = \"reviews.csv\"\n",
        "\n",
        "# Download the CSV file\n",
        "print(f\"Downloading {REVIEWS_CSV_PATH} to {LOCAL_REVIEWS_CSV_PATH}...\")\n",
        "response = requests.get(REVIEWS_CSV_PATH)\n",
        "response.raise_for_status() # Raise an exception for bad status codes\n",
        "with open(LOCAL_REVIEWS_CSV_PATH, 'wb') as f:\n",
        "    f.write(response.content)\n",
        "print(\"Download complete.\")\n",
        "\n",
        "\n",
        "# Create an instance of the CSVLoader.\n",
        "loader = CSVLoader(\n",
        "    file_path=LOCAL_REVIEWS_CSV_PATH,  # Specify the path to the downloaded CSV file to be loaded.\n",
        "    source_column=\"review\"       # Specify the name of the column that contains the main text content.\n",
        ")\n",
        "\n",
        "# Call the .load() method on the loader instance.\n",
        "# This reads the specified column from the CSV file and loads the content into a list of Document objects.\n",
        "reviews = loader.load()\n",
        "\n",
        "# Specify the embedding function to use. We define it once to be reused.\n",
        "embedding_function = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/gemini-embedding-001\",  # Choose the specific embedding model provided by Google.\n",
        "    google_api_key=userdata.get('GOOGLE_API_KEY')  # Securely fetch the Google API key.\n",
        ")\n",
        "\n",
        "# Set the size of each batch to process.\n",
        "batch_size = 20\n",
        "# Calculate the total number of batches.\n",
        "num_batches = (len(reviews) - 1) // batch_size + 1\n",
        "reviews_vector_db = None\n",
        "\n",
        "# Loop through the documents in batches to avoid hitting the API's rate limit.\n",
        "for i in range(0, len(reviews), batch_size):\n",
        "    # Get the current batch of documents.\n",
        "    batch_docs = reviews[i:i + batch_size]\n",
        "    current_batch_num = i // batch_size + 1\n",
        "\n",
        "    print(f\"Processing batch {current_batch_num}/{num_batches}...\")\n",
        "\n",
        "    if i == 0:\n",
        "        # For the first batch, create a new Chroma vector database.\n",
        "        # The `from_documents` method handles the entire process of embedding and storing the data.\n",
        "        reviews_vector_db = Chroma.from_documents(\n",
        "            documents=batch_docs,  # Pass the list of Document objects that need to be embedded.\n",
        "            embedding=embedding_function,\n",
        "            # Specify the directory on the disk where the vector database will be saved.\n",
        "            # This makes the database persistent, so we can load it directly in the future.\n",
        "            persist_directory=REVIEWS_CHROMA_PATH\n",
        "        )\n",
        "    else:\n",
        "        # For subsequent batches, add the documents to the existing database.\n",
        "        reviews_vector_db.add_documents(documents=batch_docs)\n",
        "\n",
        "    # Pause the script for 30 seconds after each batch to respect the per-minute rate limit.\n",
        "    print(f\"Batch {current_batch_num} processed. Waiting for 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "print(\"Vector database created successfully and saved to the specified directory.\")"
      ],
      "metadata": {
        "id": "8vrBwvI9YwTr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301bdd65-7abb-4634-bcd9-2949f0e5ef34"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://raw.githubusercontent.com/GeeksforgeeksDS/21-Days-21-Projects-Dataset/main/Datasets/reviews.csv to reviews.csv...\n",
            "Download complete.\n",
            "Processing batch 1/51...\n",
            "Batch 1 processed. Waiting for 30 seconds...\n",
            "Processing batch 2/51...\n",
            "Batch 2 processed. Waiting for 30 seconds...\n",
            "Processing batch 3/51...\n",
            "Batch 3 processed. Waiting for 30 seconds...\n",
            "Processing batch 4/51...\n",
            "Batch 4 processed. Waiting for 30 seconds...\n",
            "Processing batch 5/51...\n",
            "Batch 5 processed. Waiting for 30 seconds...\n",
            "Processing batch 6/51...\n",
            "Batch 6 processed. Waiting for 30 seconds...\n",
            "Processing batch 7/51...\n",
            "Batch 7 processed. Waiting for 30 seconds...\n",
            "Processing batch 8/51...\n",
            "Batch 8 processed. Waiting for 30 seconds...\n",
            "Processing batch 9/51...\n",
            "Batch 9 processed. Waiting for 30 seconds...\n",
            "Processing batch 10/51...\n",
            "Batch 10 processed. Waiting for 30 seconds...\n",
            "Processing batch 11/51...\n",
            "Batch 11 processed. Waiting for 30 seconds...\n",
            "Processing batch 12/51...\n",
            "Batch 12 processed. Waiting for 30 seconds...\n",
            "Processing batch 13/51...\n",
            "Batch 13 processed. Waiting for 30 seconds...\n",
            "Processing batch 14/51...\n",
            "Batch 14 processed. Waiting for 30 seconds...\n",
            "Processing batch 15/51...\n",
            "Batch 15 processed. Waiting for 30 seconds...\n",
            "Processing batch 16/51...\n",
            "Batch 16 processed. Waiting for 30 seconds...\n",
            "Processing batch 17/51...\n",
            "Batch 17 processed. Waiting for 30 seconds...\n",
            "Processing batch 18/51...\n",
            "Batch 18 processed. Waiting for 30 seconds...\n",
            "Processing batch 19/51...\n",
            "Batch 19 processed. Waiting for 30 seconds...\n",
            "Processing batch 20/51...\n",
            "Batch 20 processed. Waiting for 30 seconds...\n",
            "Processing batch 21/51...\n",
            "Batch 21 processed. Waiting for 30 seconds...\n",
            "Processing batch 22/51...\n",
            "Batch 22 processed. Waiting for 30 seconds...\n",
            "Processing batch 23/51...\n",
            "Batch 23 processed. Waiting for 30 seconds...\n",
            "Processing batch 24/51...\n",
            "Batch 24 processed. Waiting for 30 seconds...\n",
            "Processing batch 25/51...\n",
            "Batch 25 processed. Waiting for 30 seconds...\n",
            "Processing batch 26/51...\n",
            "Batch 26 processed. Waiting for 30 seconds...\n",
            "Processing batch 27/51...\n",
            "Batch 27 processed. Waiting for 30 seconds...\n",
            "Processing batch 28/51...\n",
            "Batch 28 processed. Waiting for 30 seconds...\n",
            "Processing batch 29/51...\n",
            "Batch 29 processed. Waiting for 30 seconds...\n",
            "Processing batch 30/51...\n",
            "Batch 30 processed. Waiting for 30 seconds...\n",
            "Processing batch 31/51...\n",
            "Batch 31 processed. Waiting for 30 seconds...\n",
            "Processing batch 32/51...\n",
            "Batch 32 processed. Waiting for 30 seconds...\n",
            "Processing batch 33/51...\n",
            "Batch 33 processed. Waiting for 30 seconds...\n",
            "Processing batch 34/51...\n",
            "Batch 34 processed. Waiting for 30 seconds...\n",
            "Processing batch 35/51...\n",
            "Batch 35 processed. Waiting for 30 seconds...\n",
            "Processing batch 36/51...\n",
            "Batch 36 processed. Waiting for 30 seconds...\n",
            "Processing batch 37/51...\n",
            "Batch 37 processed. Waiting for 30 seconds...\n",
            "Processing batch 38/51...\n",
            "Batch 38 processed. Waiting for 30 seconds...\n",
            "Processing batch 39/51...\n",
            "Batch 39 processed. Waiting for 30 seconds...\n",
            "Processing batch 40/51...\n",
            "Batch 40 processed. Waiting for 30 seconds...\n",
            "Processing batch 41/51...\n",
            "Batch 41 processed. Waiting for 30 seconds...\n",
            "Processing batch 42/51...\n",
            "Batch 42 processed. Waiting for 30 seconds...\n",
            "Processing batch 43/51...\n",
            "Batch 43 processed. Waiting for 30 seconds...\n",
            "Processing batch 44/51...\n",
            "Batch 44 processed. Waiting for 30 seconds...\n",
            "Processing batch 45/51...\n",
            "Batch 45 processed. Waiting for 30 seconds...\n",
            "Processing batch 46/51...\n",
            "Batch 46 processed. Waiting for 30 seconds...\n",
            "Processing batch 47/51...\n",
            "Batch 47 processed. Waiting for 30 seconds...\n",
            "Processing batch 48/51...\n",
            "Batch 48 processed. Waiting for 30 seconds...\n",
            "Processing batch 49/51...\n",
            "Batch 49 processed. Waiting for 30 seconds...\n",
            "Processing batch 50/51...\n",
            "Batch 50 processed. Waiting for 30 seconds...\n",
            "Processing batch 51/51...\n",
            "Batch 51 processed. Waiting for 30 seconds...\n",
            "Vector database created successfully and saved to the specified directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieval"
      ],
      "metadata": {
        "id": "M0082YQZi7dG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "relevant_chunks = reviews_vector_db.similarity_search(question, k=3)\n",
        "\n",
        "relevant_chunks[0].page_content"
      ],
      "metadata": {
        "id": "FEKxBKTNi7LZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "ea3a00e5-d7ea-4bb4-eb9d-8951003c5470"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "GoogleGenerativeAIError",
          "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[1;32m    323\u001b[0m             )\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEmbedContentResponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36membed_content\u001b[0;34m(self, request, model, content, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m   1307\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1572730773.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrelevant_chunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreviews_vector_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mrelevant_chunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \"\"\"\n\u001b[0;32m--> 698\u001b[0;31m         docs_and_scores = self.similarity_search_with_score(\n\u001b[0m\u001b[1;32m    699\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m             )\n\u001b[1;32m    797\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m             \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m             results = self.__query_collection(\n\u001b[1;32m    800\u001b[0m                 \u001b[0mquery_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Error embedding content: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGoogleGenerativeAIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGoogleGenerativeAIError\u001b[0m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_chunks[1].page_content"
      ],
      "metadata": {
        "id": "qrmyucMmffPJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "90c079c3-afde-4e98-8e17-5de02d019f87"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'relevant_chunks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-874626907.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelevant_chunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'relevant_chunks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "relevant_chunks[2].page_content"
      ],
      "metadata": {
        "id": "NeYvAtnSjATn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "f1efd785-d0ac-4347-80ac-26d3c870a528"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'relevant_chunks' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-70098108.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrelevant_chunks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'relevant_chunks' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema.runnable import RunnablePassthrough  # Allows passing inputs through unchanged in a pipeline\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses the model's output into a clean string\n",
        "\n",
        "# Create a retriever to fetch the top 10 most relevant reviews based on a query\n",
        "reviews_retriever = reviews_vector_db.as_retriever(k=10)\n",
        "# The `as_retriever` method converts the database into a retriever.\n",
        "# `k=10` specifies that the retriever should return the top 10 most relevant documents for a query.\n",
        "\n",
        "# Create a chain for querying and generating responses\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    # Step 1: Retrieves relevant reviews (`context`) and passes the `question` unchanged\n",
        "    | review_prompt_template\n",
        "    # Step 2: Formats the retrieved reviews and the user's question into a structured prompt\n",
        "    | chat_model\n",
        "    # Step 3: Sends the prompt to the Gemini chat model to generate a response\n",
        "    | StrOutputParser()\n",
        "    # Step 4: Parses the model's raw output into a clean string format for easier use\n",
        ")"
      ],
      "metadata": {
        "id": "L_-CkirwquWu"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "review_chain.invoke(question)"
      ],
      "metadata": {
        "id": "tkRCoPB5ro7_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "d1d3fb7a-6e01-4eb3-e6a5-6b648fb77da7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "GoogleGenerativeAIError",
          "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[1;32m    323\u001b[0m             )\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEmbedContentResponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36membed_content\u001b[0;34m(self, request, model, content, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m   1307\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3916360560.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreview_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3242\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3243\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3244\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3245\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3246\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3999\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m                 ]\n\u001b[0;32m-> 4001\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4002\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(step, input_, config, key)\u001b[0m\n\u001b[1;32m   3983\u001b[0m             )\n\u001b[1;32m   3984\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3985\u001b[0;31m                 return context.run(\n\u001b[0m\u001b[1;32m   3986\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3987\u001b[0m                     \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_other_args\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 result = self._get_relevant_documents(\n\u001b[0m\u001b[1;32m    264\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36m_get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity_score_threshold\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             docs_and_similarities = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \"\"\"\n\u001b[0;32m--> 698\u001b[0;31m         docs_and_scores = self.similarity_search_with_score(\n\u001b[0m\u001b[1;32m    699\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m             )\n\u001b[1;32m    797\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m             \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m             results = self.__query_collection(\n\u001b[1;32m    800\u001b[0m                 \u001b[0mquery_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Error embedding content: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGoogleGenerativeAIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGoogleGenerativeAIError\u001b[0m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Putting it all together"
      ],
      "metadata": {
        "id": "0FG3EW1-laiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import (\n",
        "    PromptTemplate,  # Template for formatting prompts with dynamic variables\n",
        "    SystemMessagePromptTemplate,  # Represents a system-level instruction to the model\n",
        "    HumanMessagePromptTemplate,  # Represents a human-level input for the model\n",
        "    ChatPromptTemplate,  # Combines multiple prompt components into a unified chat prompt\n",
        ")\n",
        "\n",
        "# Define the system prompt template as a string with placeholders for dynamic content\n",
        "review_template_str = \"\"\"Your job is to use patient reviews to answer questions about their experience at a hospital.\n",
        "Use the following context to answer questions.\n",
        "Be as detailed as possible, but don't make up any information that's not from the context.\n",
        "If you don't know an answer, say you don't know.\n",
        "\n",
        "{context}\n",
        "\"\"\"\n",
        "\n",
        "# Create a system-level message prompt template for the chatbot\n",
        "review_system_prompt = SystemMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"context\"],  # Placeholder for the \"context\" (e.g., patient reviews)\n",
        "        template=review_template_str,  # The instructions and structure of the system prompt\n",
        "    )\n",
        ")\n",
        "\n",
        "# Create a human-level message prompt template for user input\n",
        "review_human_prompt = HumanMessagePromptTemplate(\n",
        "    prompt=PromptTemplate(\n",
        "        input_variables=[\"question\"],  # Placeholder for the \"question\" to be answered\n",
        "        template=\"{question}\",  # A simple template where the \"question\" is dynamically inserted\n",
        "    )\n",
        ")\n",
        "\n",
        "# Combine the system and human prompts into a list of messages\n",
        "messages = [review_system_prompt, review_human_prompt]\n",
        "\n",
        "# Create a chat prompt template that integrates the system and human prompts\n",
        "review_prompt_template = ChatPromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],  # Define the expected inputs for the template\n",
        "    messages=messages,  # Combine the individual prompt components (system and human)\n",
        ")"
      ],
      "metadata": {
        "id": "uKaKX6pglaVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "chat_model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\",\n",
        "                                  temperature=0,\n",
        "                                  google_api_key=userdata.get('GOOGLE_API_KEY'))"
      ],
      "metadata": {
        "id": "7PFV7mkcjB5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing required modules and classes\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.prompts import (\n",
        "    PromptTemplate,  # Template for structuring prompts\n",
        "    SystemMessagePromptTemplate,  # System-level instructions for the model\n",
        "    HumanMessagePromptTemplate,  # Human input instructions for the model\n",
        "    ChatPromptTemplate,  # Combines system and human prompts into a single chat prompt\n",
        ")\n",
        "from langchain_core.output_parsers import StrOutputParser  # Parses the model's output into a clean string\n",
        "from langchain_community.vectorstores import Chroma  # Vector database for efficient similarity searches\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings  # Converts text to embeddings using Google's API\n",
        "from langchain.schema.runnable import RunnablePassthrough  # Allows passing inputs through unchanged in a pipeline\n",
        "\n",
        "# Path to the persistent Chroma vector database\n",
        "REVIEWS_CHROMA_PATH = \"chroma_data\"\n",
        "\n",
        "# Specify the embedding function to use. We define it once to be reused.\n",
        "embedding_function = GoogleGenerativeAIEmbeddings(\n",
        "    model=\"models/embedding-001\",  # Choose the specific embedding model provided by Google.\n",
        "    google_api_key=userdata.get('GOOGLE_API_KEY')  # Securely fetch the Google API key.\n",
        ")\n",
        "\n",
        "# Set the size of each batch to process.\n",
        "batch_size = 20\n",
        "# Calculate the total number of batches.\n",
        "num_batches = (len(reviews) - 1) // batch_size + 1\n",
        "reviews_vector_db = None\n",
        "\n",
        "# Loop through the documents in batches to avoid hitting the API's rate limit.\n",
        "for i in range(0, len(reviews), batch_size):\n",
        "    # Get the current batch of documents.\n",
        "    batch_docs = reviews[i:i + batch_size]\n",
        "    current_batch_num = i // batch_size + 1\n",
        "\n",
        "    print(f\"Processing batch {current_batch_num}/{num_batches}...\")\n",
        "\n",
        "    if i == 0:\n",
        "        # For the first batch, create a new Chroma vector database.\n",
        "        # The `from_documents` method handles the entire process of embedding and storing the data.\n",
        "        reviews_vector_db = Chroma.from_documents(\n",
        "            documents=batch_docs,  # Pass the list of Document objects that need to be embedded.\n",
        "            embedding=embedding_function,\n",
        "            # Specify the directory on the disk where the vector database will be saved.\n",
        "            # This makes the database persistent, so we can load it directly in the future.\n",
        "            persist_directory=REVIEWS_CHROMA_PATH\n",
        "        )\n",
        "    else:\n",
        "        # For subsequent batches, add the documents to the existing database.\n",
        "        reviews_vector_db.add_documents(documents=batch_docs)\n",
        "\n",
        "    # Pause the script for 60 seconds after each batch to respect the per-minute rate limit.\n",
        "    print(f\"Batch {current_batch_num} processed. Waiting for 30 seconds...\")\n",
        "    time.sleep(30)\n",
        "\n",
        "print(\"Vector database created successfully and saved to the specified directory.\")"
      ],
      "metadata": {
        "id": "5CTUzRoNlvDY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever to fetch the top 10 most relevant reviews based on a query\n",
        "reviews_retriever = reviews_vector_db.as_retriever(k=10)\n",
        "# The `as_retriever` method converts the database into a retriever.\n",
        "# `k=10` specifies that the retriever should return the top 10 most relevant documents for a query.\n",
        "\n",
        "# Create a chain for querying and generating responses\n",
        "review_chain = (\n",
        "    {\"context\": reviews_retriever, \"question\": RunnablePassthrough()}\n",
        "    # Step 1: Retrieves relevant reviews (`context`) and passes the `question` unchanged\n",
        "    | review_prompt_template\n",
        "    # Step 2: Formats the retrieved reviews and the user's question into a structured prompt\n",
        "    | chat_model\n",
        "    # Step 3: Sends the prompt to the OpenAI chat model to generate a response\n",
        "    | StrOutputParser()\n",
        "    # Step 4: Parses the model's raw output into a clean string format for easier use\n",
        ")"
      ],
      "metadata": {
        "id": "PV6lQyYDl7WD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Has anyone complained about communication with the hospital staff?\"\"\"\n",
        "review_chain.invoke(question)"
      ],
      "metadata": {
        "id": "VSDO_5aymCIr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Adding a UI"
      ],
      "metadata": {
        "id": "3J_-TawemeDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "id": "yL-EnjFRmdrb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a763db86-6fdf-406a-fdc6-bbfa35aec87f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.46.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.10.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.116.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.13.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.13.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.11.9)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.0)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.17.4)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.15.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.13.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (3.19.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (2.32.5)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.33.5->gradio) (1.1.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<1.0,>=0.33.5->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def respond_to_user_question(question: str, history: list) -> str:\n",
        "    \"\"\"\n",
        "    Respond to a user's question using the review_chain.\n",
        "    \"\"\"\n",
        "    return review_chain.invoke(question)"
      ],
      "metadata": {
        "id": "BnkjE9zXmgqT"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "respond_to_user_question(\"Has anyone complained about communication with the hospital staff?\", [])"
      ],
      "metadata": {
        "id": "Po1hiTd6mko-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "cab9a626-601b-4098-d37a-f47c0029993c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "GoogleGenerativeAIError",
          "evalue": "Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhausted\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[1;32m    323\u001b[0m             )\n\u001b[0;32m--> 324\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mEmbedContentResponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\u001b[0m in \u001b[0;36membed_content\u001b[0;34m(self, request, model, content, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m         \u001b[0;31m# Send the request.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m         response = rpc(\n\u001b[0m\u001b[1;32m   1307\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    295\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;31m# defer to shared logic for handling errors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             next_sleep = _retry_error_helper(\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mexc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\u001b[0m in \u001b[0;36m_retry_error_helper\u001b[0;34m(exc, deadline, sleep_iterator, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)\u001b[0m\n\u001b[1;32m    213\u001b[0m         )\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfinal_exc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msource_exc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mon_error_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misawaitable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mResourceExhausted\u001b[0m: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mGoogleGenerativeAIError\u001b[0m                   Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3864091116.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrespond_to_user_question\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Has anyone complained about communication with the hospital staff?\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3877995097.py\u001b[0m in \u001b[0;36mrespond_to_user_question\u001b[0;34m(question, history)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mRespond\u001b[0m \u001b[0mto\u001b[0m \u001b[0ma\u001b[0m \u001b[0muser\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mquestion\u001b[0m \u001b[0musing\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreview_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \"\"\"\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mreview_chain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3242\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3243\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3244\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3245\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3246\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3999\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4000\u001b[0m                 ]\n\u001b[0;32m-> 4001\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4002\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(step, input_, config, key)\u001b[0m\n\u001b[1;32m   3983\u001b[0m             )\n\u001b[1;32m   3984\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3985\u001b[0;31m                 return context.run(\n\u001b[0m\u001b[1;32m   3986\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3987\u001b[0m                     \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/retrievers.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m             \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expects_other_args\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_arg_supported\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m                 result = self._get_relevant_documents(\n\u001b[0m\u001b[1;32m    264\u001b[0m                     \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_core/vectorstores/base.py\u001b[0m in \u001b[0;36m_get_relevant_documents\u001b[0;34m(self, query, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         \u001b[0mkwargs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_kwargs\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m             \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimilarity_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"similarity_score_threshold\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             docs_and_similarities = (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search\u001b[0;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[1;32m    696\u001b[0m             \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0mmost\u001b[0m \u001b[0msimilar\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mquery\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m         \"\"\"\n\u001b[0;32m--> 698\u001b[0;31m         docs_and_scores = self.similarity_search_with_score(\n\u001b[0m\u001b[1;32m    699\u001b[0m             \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m             \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\u001b[0m in \u001b[0;36msimilarity_search_with_score\u001b[0;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[1;32m    796\u001b[0m             )\n\u001b[1;32m    797\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m             \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m             results = self.__query_collection(\n\u001b[1;32m    800\u001b[0m                 \u001b[0mquery_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\u001b[0m in \u001b[0;36membed_query\u001b[0;34m(self, text, task_type, title, output_dimensionality)\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Error embedding content: {e}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mGoogleGenerativeAIError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mGoogleGenerativeAIError\u001b[0m: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n}\n, links {\n  description: \"Learn more about Gemini API quotas\"\n  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n}\n]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "# Create the Gradio ChatInterface\n",
        "interface = gr.ChatInterface(fn=respond_to_user_question, title=\"Review Helper Bot\")\n",
        "\n",
        "# Launch the Gradio app\n",
        "interface.launch(debug=True)"
      ],
      "metadata": {
        "id": "3vBIk8T_mkln",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c1143a15-9dea-43ab-edf8-001fe2891a71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:348: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://5f4018b403ee1f5cb7.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://5f4018b403ee1f5cb7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\", line 324, in embed_query\n",
            "    result: EmbedContentResponse = self.client.embed_content(request)\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py\", line 1306, in embed_content\n",
            "    response = rpc(\n",
            "               ^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/gapic_v1/method.py\", line 131, in __call__\n",
            "    return wrapped_func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 294, in retry_wrapped_func\n",
            "    return retry_target(\n",
            "           ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 156, in retry_target\n",
            "    next_sleep = _retry_error_helper(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_base.py\", line 214, in _retry_error_helper\n",
            "    raise final_exc from source_exc\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/retry/retry_unary.py\", line 147, in retry_target\n",
            "    result = target()\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/timeout.py\", line 130, in func_with_timeout\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/api_core/grpc_helpers.py\", line 78, in error_remapped_callable\n",
            "    raise exceptions.from_grpc_error(exc) from exc\n",
            "google.api_core.exceptions.ResourceExhausted: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n",
            "  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            "]\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/queueing.py\", line 745, in process_events\n",
            "    response = await route_utils.call_process_api(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/route_utils.py\", line 353, in call_process_api\n",
            "    output = await app.get_blocks().process_api(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 2116, in process_api\n",
            "    result = await self.call_function(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 1621, in call_function\n",
            "    prediction = await fn(*processed_input)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/utils.py\", line 882, in async_wrapper\n",
            "    response = await f(*args, **kwargs)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 554, in __wrapper\n",
            "    return await submit_fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py\", line 944, in _submit_fn\n",
            "    response = await anyio.to_thread.run_sync(\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/to_thread.py\", line 56, in run_sync\n",
            "    return await get_async_backend().run_sync_in_worker_thread(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
            "    return await future\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
            "    result = context.run(func, *args)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/tmp/ipython-input-3877995097.py\", line 5, in respond_to_user_question\n",
            "    return review_chain.invoke(question)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3244, in invoke\n",
            "    input_ = context.run(step.invoke, input_, config, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 4001, in invoke\n",
            "    output = {key: future.result() for key, future in zip(steps, futures)}\n",
            "                   ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 456, in result\n",
            "    return self.__get_result()\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/_base.py\", line 401, in __get_result\n",
            "    raise self._exception\n",
            "  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 59, in run\n",
            "    result = self.fn(*self.args, **self.kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/runnables/base.py\", line 3985, in _invoke_step\n",
            "    return context.run(\n",
            "           ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/retrievers.py\", line 263, in invoke\n",
            "    result = self._get_relevant_documents(\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_core/vectorstores/base.py\", line 1067, in _get_relevant_documents\n",
            "    docs = self.vectorstore.similarity_search(query, **kwargs_)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\", line 698, in similarity_search\n",
            "    docs_and_scores = self.similarity_search_with_score(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_chroma/vectorstores.py\", line 798, in similarity_search_with_score\n",
            "    query_embedding = self._embedding_function.embed_query(query)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/langchain_google_genai/embeddings.py\", line 327, in embed_query\n",
            "    raise GoogleGenerativeAIError(msg) from e\n",
            "langchain_google_genai._common.GoogleGenerativeAIError: Error embedding content: 429 You exceeded your current quota, please check your plan and billing details. For more information on this error, head to: https://ai.google.dev/gemini-api/docs/rate-limits.\n",
            "* Quota exceeded for metric: generativelanguage.googleapis.com/embed_content_free_tier_requests, limit: 0 [violations {\n",
            "  quota_metric: \"generativelanguage.googleapis.com/embed_content_free_tier_requests\"\n",
            "  quota_id: \"EmbedContentRequestsPerDayPerUserPerProjectPerModel-FreeTier\"\n",
            "}\n",
            ", links {\n",
            "  description: \"Learn more about Gemini API quotas\"\n",
            "  url: \"https://ai.google.dev/gemini-api/docs/rate-limits\"\n",
            "}\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bl7W3EFfuhvT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TsUpDckHug1H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your goal is to get one incorrect answer, or 5 really amazing answers."
      ],
      "metadata": {
        "id": "f-Zb_TKiNkaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submit screenshots"
      ],
      "metadata": {
        "id": "I0GVCD-jNwra"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LziiVwG0A9YZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}